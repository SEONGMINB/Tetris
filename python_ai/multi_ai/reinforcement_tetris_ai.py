#!/usr/bin/env python3
"""
PyTorch DQN ê¸°ë°˜ ë‹¤ì¤‘ ì „ë¬¸í™” í…ŒíŠ¸ë¦¬ìŠ¤ AI ì‹œìŠ¤í…œ
- HoleFindingAI: êµ¬ë© ë©”ìš°ê¸° ì „ë¬¸
- ShapeOptimizingAI: ìµœì  í˜•íƒœ ìœ ì§€ ì „ë¬¸  
- StackOptimizingAI: ê· ë“±í•œ ë†’ì´ ìœ ì§€ ì „ë¬¸
- LineClearingAI: ë‚®ì€ ì¸µ ì™„ì„± ì „ë¬¸
- StrategicAI: ë©€í‹°ë¼ì¸ í´ë¦¬ì–´ ì „ë¬¸

ğŸ†• ìŠ¤ë§ˆíŠ¸ ì•™ìƒë¸” ëª¨ë“œ: ìš°ì„ ìˆœìœ„ ê¸°ë°˜ AI ì„ íƒ ì‹œìŠ¤í…œ
- ìœ„í—˜ë„ ë¶„ì„: ê²Œì„ ì˜¤ë²„ ìœ„í—˜ì„± í‰ê°€
- ê³µê°„ í™œìš©ë„: ë„“ì€ ê³µê°„ í™œìš© ìµœì í™”
- í˜•íƒœ ì í•©ì„±: í˜„ì¬ ë¸”ë¡ê³¼ ë³´ë“œì˜ ì¡°í™”
- ìƒì¡´ ìš°ì„  ì „ëµ: ìœ„í—˜ ìƒí™©ì—ì„œ ìƒì¡´ ì•¡ì…˜ ìš°ì„  ì„ íƒ
- ìƒí™©ë³„ ìš°ì„ ìˆœìœ„: ì•ˆì „/ìœ„í—˜ ìƒí™©ì— ë”°ë¥¸ AI ì—­í•  ì¡°ì •
"""
import requests
import time
import numpy as np
import random
from collections import deque
import json
import os

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    PYTORCH_AVAILABLE = True
    print("âœ… PyTorch ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    PYTORCH_AVAILABLE = False
    print("âŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. 'pip install torch' ì‹¤í–‰ í•„ìš”")

# PyTorch DQN ë„¤íŠ¸ì›Œí¬
class DQN(nn.Module):
    def __init__(self, input_size, hidden_size=256, output_size=5):
        super(DQN, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, output_size)
        )
    
    def forward(self, x):
        return self.network(x)

# ê¸°ë³¸ DQN ì—ì´ì „íŠ¸
class BaseDQNAgent:
    def __init__(self, input_size, action_size=5, learning_rate=0.001, 
                 discount_factor=0.95, epsilon=1.0, memory_size=10000):
        self.input_size = input_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        
        # PyTorch ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self.q_network = DQN(input_size, output_size=action_size).to(self.device)
        self.target_network = DQN(input_size, output_size=action_size).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # ë©”ëª¨ë¦¬
        self.memory = deque(maxlen=memory_size)
        self.update_target_frequency = 100
        self.step_count = 0
        
        # ì•¡ì…˜ ë§¤í•‘ (restartëŠ” ì œì™¸í•˜ê³  ê²Œì„ ì˜¤ë²„ ì‹œì—ë§Œ ìˆ˜ë™ìœ¼ë¡œ ì²˜ë¦¬)
        self.actions = ['left', 'right', 'down', 'rotate', 'drop']
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self.update_target_network()
    
    def extract_features(self, state):
        """ê¸°ë³¸ íŠ¹ì§• ì¶”ì¶œ - ìƒì†ë°›ëŠ” í´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ"""
        if not state or 'board' not in state:
            return np.zeros(self.input_size)
        
        board = state['board']
        features = []
        
        # ê¸°ë³¸ íŠ¹ì§•ë“¤
        features.extend(self._get_height_features(board))
        features.extend(self._get_hole_features(board))
        features.extend(self._get_line_features(board))
        features.extend(self._get_shape_features(board))
        
        # ê²Œì„ ë©”íƒ€ ì •ë³´
        features.append(state.get('score', 0) / 10000.0)
        features.append(state.get('lines', 0) / 100.0)
        features.append(state.get('level', 1) / 20.0)
        
        return np.array(features[:self.input_size], dtype=np.float32)
    
    def _get_height_features(self, board):
        """ë†’ì´ ê´€ë ¨ íŠ¹ì§•"""
        heights = []
        for col in range(10):
            height = 0
            for row in range(20):
                if board[row][col] is not None and board[row][col] != 0:
                    height = 20 - row
                    break
            heights.append(height / 20.0)  # ì •ê·œí™”
        return heights
    
    def _get_hole_features(self, board):
        """êµ¬ë© ê´€ë ¨ íŠ¹ì§•"""
        holes_per_col = []
        for col in range(10):
            holes = 0
            block_found = False
            for row in range(20):
                if board[row][col] is not None and board[row][col] != 0:
                    block_found = True
                elif block_found and (board[row][col] is None or board[row][col] == 0):
                    holes += 1
            holes_per_col.append(holes / 20.0)  # ì •ê·œí™”
        return holes_per_col
    
    def _get_line_features(self, board):
        """ë¼ì¸ ê´€ë ¨ íŠ¹ì§•"""
        features = []
        complete_lines = 0
        almost_complete = 0
        
        for row in range(20):
            filled_cells = sum(1 for cell in board[row] if cell is not None and cell != 0)
            if filled_cells == 10:
                complete_lines += 1
            elif filled_cells >= 8:
                almost_complete += 1
        
        features.append(complete_lines / 4.0)  # ìµœëŒ€ 4ë¼ì¸
        features.append(almost_complete / 10.0)
        return features
    
    def _get_shape_features(self, board):
        """í˜•íƒœ ê´€ë ¨ íŠ¹ì§•"""
        features = []
        
        # í‘œë©´ ê±°ì¹ ê¸°
        heights = [0] * 10
        for col in range(10):
            for row in range(20):
                if board[row][col] is not None and board[row][col] != 0:
                    heights[col] = 20 - row
                    break
        
        roughness = sum(abs(heights[i] - heights[i+1]) for i in range(9))
        features.append(roughness / 100.0)
        
        # ìµœëŒ€ ë†’ì´
        max_height = max(heights) if heights else 0
        features.append(max_height / 20.0)
        
        return features
    
    def get_q_values(self, state):
        """í˜„ì¬ ìƒíƒœì—ì„œ ëª¨ë“  ì•¡ì…˜ì˜ Q-values ë°˜í™˜"""
        features = self.extract_features(state)
        state_tensor = torch.FloatTensor(features).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            q_values = self.q_network(state_tensor)
            return q_values.squeeze().cpu().numpy()
    
    def get_best_q_value(self, state):
        """í˜„ì¬ ìƒíƒœì—ì„œ ìµœê³  Q-value ë°˜í™˜"""
        q_values = self.get_q_values(state)
        return np.max(q_values)
    
    def get_action(self, state):
        """ì•¡ì…˜ ì„ íƒ"""
        if np.random.random() <= self.epsilon:
            return random.choice(range(self.action_size))
        
        features = self.extract_features(state)
        state_tensor = torch.FloatTensor(features).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            q_values = self.q_network(state_tensor)
            return q_values.argmax().item()
    
    def remember(self, state, action, reward, next_state, done):
        """ê²½í—˜ ì €ì¥"""
        state_features = self.extract_features(state)
        next_state_features = self.extract_features(next_state)
        self.memory.append((state_features, action, reward, next_state_features, done))
    
    def calculate_reward(self, prev_state, current_state):
        """ê¸°ë³¸ ë³´ìƒ ê³„ì‚° (ê· í˜•ì¡íŒ í”Œë ˆì´ ìœ ë„) - ìƒì†ë°›ëŠ” í´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ"""
        if not prev_state or not current_state:
            return 0
        
        reward = 0
        
        # ì ìˆ˜ ì¦ê°€ ë³´ìƒ
        score_diff = current_state.get('score', 0) - prev_state.get('score', 0)
        reward += score_diff * 0.01
        
        # ë¼ì¸ í´ë¦¬ì–´ ë³´ìƒ
        lines_diff = current_state.get('lines', 0) - prev_state.get('lines', 0)
        reward += lines_diff * 10
        
        # ê¸°ë³¸ì ì¸ ê· í˜• ìœ ì§€ ë³´ìƒ
        curr_board = current_state.get('board', [])
        if curr_board:
            heights = [0] * 10
            for col in range(10):
                for row in range(20):
                    if curr_board[row][col] is not None and curr_board[row][col] != 0:
                        heights[col] = 20 - row
                        break
            
            # ë†’ì´ ë¶„ì‚°ì´ ì ì„ ë•Œ ë³´ìƒ
            if heights:
                height_variance = sum((h - sum(heights)/len(heights))**2 for h in heights) / len(heights)
                if height_variance < 15:
                    reward += (15 - height_variance) * 0.5
                
                # ê·¹ë‹¨ì  ë†’ì´ ì°¨ì´ íŒ¨ë„í‹°
                height_diff = max(heights) - min(heights)
                if height_diff > 12:
                    reward -= (height_diff - 12) * 2
        
        # ê²Œì„ ì˜¤ë²„ íŒ¨ë„í‹°
        if current_state.get('isGameOver', False):
            reward -= 50
        
        return reward
    
    def replay(self, batch_size=32):
        """ê²½í—˜ ì¬ìƒ í•™ìŠµ"""
        if len(self.memory) < batch_size:
            return
        
        batch = random.sample(self.memory, batch_size)
        
        states = torch.FloatTensor([e[0] for e in batch]).to(self.device)
        actions = torch.LongTensor([e[1] for e in batch]).to(self.device)
        rewards = torch.FloatTensor([e[2] for e in batch]).to(self.device)
        next_states = torch.FloatTensor([e[3] for e in batch]).to(self.device)
        dones = torch.BoolTensor([e[4] for e in batch]).to(self.device)
        
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        with torch.no_grad():
            next_q_values = self.target_network(next_states).max(1)[0]
            target_q_values = rewards + (self.discount_factor * next_q_values * ~dones)
        
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()
        
        # Epsilon ê°ì†Œ
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
        self.step_count += 1
        if self.step_count % self.update_target_frequency == 0:
            self.update_target_network()
    
    def update_target_network(self):
        """íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def save_model(self, filepath):
        """ëª¨ë¸ ì €ì¥"""
        torch.save({
            'q_network_state_dict': self.q_network.state_dict(),
            'target_network_state_dict': self.target_network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'step_count': self.step_count
        }, filepath)
    
    def load_model(self, filepath):
        """ëª¨ë¸ ë¡œë“œ (ì°¨ì› ë¶ˆì¼ì¹˜ ì‹œ ìƒˆë¡œ ì´ˆê¸°í™”)"""
        if os.path.exists(filepath):
            try:
                checkpoint = torch.load(filepath, map_location=self.device)
                
                # ì°¨ì› í˜¸í™˜ì„± ê²€ì‚¬
                q_network_state = checkpoint['q_network_state_dict']
                current_input_size = self.q_network.network[0].in_features
                saved_input_size = q_network_state['network.0.weight'].shape[1]
                
                if current_input_size != saved_input_size:
                    print(f"âš ï¸  ëª¨ë¸ ì°¨ì› ë¶ˆì¼ì¹˜: ì €ì¥ëœ ëª¨ë¸ ì…ë ¥ í¬ê¸°({saved_input_size}) != í˜„ì¬ ëª¨ë¸ ì…ë ¥ í¬ê¸°({current_input_size})")
                    print(f"ğŸ”„ ìƒˆ ëª¨ë¸ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤: {filepath}")
                    return
                
                self.q_network.load_state_dict(q_network_state)
                self.target_network.load_state_dict(checkpoint['target_network_state_dict'])
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                self.epsilon = checkpoint.get('epsilon', self.epsilon)
                self.step_count = checkpoint.get('step_count', 0)
                print(f"âœ… ëª¨ë¸ ë¡œë“œë¨: {filepath}")
                
            except Exception as e:
                print(f"âš ï¸  ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print(f"ğŸ”„ ìƒˆ ëª¨ë¸ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤: {filepath}")
        else:
            print(f"âŒ ì €ì¥ëœ ëª¨ë¸ ì—†ìŒ: {filepath}")

# 1. êµ¬ë© ì°¾ê¸° ì „ë¬¸ AI
class HoleFindingAI(BaseDQNAgent):
    def __init__(self):
        super().__init__(input_size=25, learning_rate=0.001)  # 10*2 + 1 + 1 + 3 = 25
        self.name = "HoleFinding"
        
    def extract_features(self, state):
        """êµ¬ë© ìœ„ì¹˜ì™€ ë©”ìš°ê¸°ì— íŠ¹í™”ëœ íŠ¹ì§•"""
        if not state or 'board' not in state:
            return np.zeros(self.input_size)
        
        board = state['board']
        features = []
        
        # ê° ì—´ì˜ êµ¬ë© ìœ„ì¹˜ ë¶„ì„
        for col in range(10):
            column_features = self._analyze_column_holes(board, col)
            features.extend(column_features)
        
        # ì „ì²´ êµ¬ë© í†µê³„
        total_holes = sum(self._count_holes_in_column(board, col) for col in range(10))
        features.append(total_holes / 50.0)  # ì •ê·œí™”
        
        # ìƒë‹¨ êµ¬ë© ê°œìˆ˜ (ë” ì¤‘ìš”)
        top_holes = sum(self._count_top_holes(board, col) for col in range(10))
        features.append(top_holes / 20.0)
        
        # ê²Œì„ ìƒíƒœ
        features.append(state.get('score', 0) / 10000.0)
        features.append(state.get('lines', 0) / 100.0)
        features.append(state.get('level', 1) / 20.0)
        
        return np.array(features[:self.input_size], dtype=np.float32)
    
    def _analyze_column_holes(self, board, col):
        """íŠ¹ì • ì—´ì˜ êµ¬ë© ë¶„ì„"""
        holes = 0
        top_hole_depth = 0
        block_found = False
        
        for row in range(20):
            if board[row][col] is not None and board[row][col] != 0:
                block_found = True
            elif block_found and (board[row][col] is None or board[row][col] == 0):
                holes += 1
                if top_hole_depth == 0:
                    top_hole_depth = row
        
        return [holes / 20.0, top_hole_depth / 20.0]
    
    def _count_holes_in_column(self, board, col):
        """ì—´ì˜ ì´ êµ¬ë© ê°œìˆ˜"""
        holes = 0
        block_found = False
        for row in range(20):
            if board[row][col] is not None and board[row][col] != 0:
                block_found = True
            elif block_found and (board[row][col] is None or board[row][col] == 0):
                holes += 1
        return holes
    
    def _count_top_holes(self, board, col):
        """ìƒë‹¨ ì ˆë°˜ì˜ êµ¬ë© ê°œìˆ˜"""
        holes = 0
        block_found = False
        for row in range(10):  # ìƒë‹¨ ì ˆë°˜ë§Œ
            if board[row][col] is not None and board[row][col] != 0:
                block_found = True
            elif block_found and (board[row][col] is None or board[row][col] == 0):
                holes += 1
        return holes
    
    def calculate_reward(self, prev_state, current_state):
        """êµ¬ë© ë©”ìš°ê¸°ì— íŠ¹í™”ëœ ë³´ìƒ"""
        if not prev_state or not current_state:
            return 0
        
        reward = 0
        
        # ê¸°ë³¸ ë³´ìƒ
        score_diff = current_state.get('score', 0) - prev_state.get('score', 0)
        reward += score_diff * 0.01
        
        # êµ¬ë© ê°ì†Œ ë³´ìƒ (ë†’ì€ ê°€ì¤‘ì¹˜)
        prev_holes = self._count_total_holes(prev_state.get('board', []))
        curr_holes = self._count_total_holes(current_state.get('board', []))
        hole_reduction = prev_holes - curr_holes
        reward += hole_reduction * 20  # êµ¬ë© í•˜ë‚˜ë‹¹ 20ì 
        
        # ìƒë‹¨ êµ¬ë© ê°ì†Œ ì¶”ê°€ ë³´ìƒ
        prev_top_holes = self._count_total_top_holes(prev_state.get('board', []))
        curr_top_holes = self._count_total_top_holes(current_state.get('board', []))
        top_hole_reduction = prev_top_holes - curr_top_holes
        reward += top_hole_reduction * 30  # ìƒë‹¨ êµ¬ë©ì€ ë” ì¤‘ìš”
        
        # ë¼ì¸ í´ë¦¬ì–´ ë³´ìƒ
        lines_diff = current_state.get('lines', 0) - prev_state.get('lines', 0)
        reward += lines_diff * 15
        
        # ê²Œì„ ì˜¤ë²„ íŒ¨ë„í‹°
        if current_state.get('isGameOver', False):
            reward -= 100
        
        return reward
    
    def _count_total_holes(self, board):
        """ì „ì²´ ë³´ë“œì˜ êµ¬ë© ê°œìˆ˜"""
        if not board:
            return 0
        return sum(self._count_holes_in_column(board, col) for col in range(10))
    
    def _count_total_top_holes(self, board):
        """ìƒë‹¨ ì ˆë°˜ì˜ ì´ êµ¬ë© ê°œìˆ˜"""
        if not board:
            return 0
        return sum(self._count_top_holes(board, col) for col in range(10))

# 2. í˜•íƒœ ìµœì í™” ì „ë¬¸ AI
class ShapeOptimizingAI(BaseDQNAgent):
    def __init__(self):
        super().__init__(input_size=29, learning_rate=0.001)  # 10 + 4 + 10 + 2 + 3 = 29
        self.name = "ShapeOptimizing"
    
    def extract_features(self, state):
        """ìµœì  í˜•íƒœ ìœ ì§€ì— íŠ¹í™”ëœ íŠ¹ì§•"""
        if not state or 'board' not in state:
            return np.zeros(self.input_size)
        
        board = state['board']
        features = []
        
        # ë†’ì´ ë¶„í¬
        heights = self._get_column_heights(board)
        features.extend([h / 20.0 for h in heights])  # 10ê°œ
        
        # í˜•íƒœ í’ˆì§ˆ ì§€í‘œ
        features.append(self._calculate_roughness(heights) / 100.0)
        features.append(self._calculate_height_variance(heights) / 100.0)
        features.append(max(heights) / 20.0 if heights else 0)
        features.append(min(heights) / 20.0 if heights else 0)
        
        # í‘œë©´ ë¶„ì„
        features.extend(self._analyze_surface_shape(board))  # 10ê°œ
        
        # ì•ˆì •ì„± ë¶„ì„
        features.append(self._calculate_stability(board))
        features.append(self._calculate_compactness(board))
        
        # ê²Œì„ ìƒíƒœ
        features.append(state.get('score', 0) / 10000.0)
        features.append(state.get('lines', 0) / 100.0)
        features.append(state.get('level', 1) / 20.0)
        
        return np.array(features[:self.input_size], dtype=np.float32)
    
    def _get_column_heights(self, board):
        """ê° ì—´ì˜ ë†’ì´"""
        heights = []
        for col in range(10):
            height = 0
            for row in range(20):
                if board[row][col] is not None and board[row][col] != 0:
                    height = 20 - row
                    break
            heights.append(height)
        return heights
    
    def _calculate_roughness(self, heights):
        """í‘œë©´ ê±°ì¹ ê¸°"""
        return sum(abs(heights[i] - heights[i+1]) for i in range(9))
    
    def _calculate_height_variance(self, heights):
        """ë†’ì´ ë¶„ì‚°"""
        if not heights:
            return 0
        mean_height = sum(heights) / len(heights)
        return sum((h - mean_height) ** 2 for h in heights) / len(heights)
    
    def _analyze_surface_shape(self, board):
        """í‘œë©´ í˜•íƒœ ë¶„ì„"""
        heights = self._get_column_heights(board)
        features = []
        
        for i in range(10):
            if i == 0:
                slope = heights[1] - heights[0] if len(heights) > 1 else 0
            elif i == 9:
                slope = heights[9] - heights[8]
            else:
                slope = (heights[i+1] - heights[i-1]) / 2
            features.append(slope / 10.0)  # ì •ê·œí™”
        
        return features
    
    def _calculate_stability(self, board):
        """êµ¬ì¡° ì•ˆì •ì„±"""
        supported_blocks = 0
        total_blocks = 0
        
        for row in range(1, 20):
            for col in range(10):
                if board[row][col] is not None and board[row][col] != 0:
                    total_blocks += 1
                    # ì•„ë˜ì— ë¸”ë¡ì´ ìˆìœ¼ë©´ ì§€ì§€ë°›ìŒ
                    if board[row+1][col] is not None and board[row+1][col] != 0:
                        supported_blocks += 1
        
        return supported_blocks / (total_blocks + 1)
    
    def _calculate_compactness(self, board):
        """êµ¬ì¡° ë°€ì§‘ë„"""
        filled_cells = 0
        total_cells = 0
        
        # ë¸”ë¡ì´ ìˆëŠ” ì˜ì—­ë§Œ ê³„ì‚°
        min_height = 20
        for col in range(10):
            for row in range(20):
                if board[row][col] is not None and board[row][col] != 0:
                    min_height = min(min_height, row)
                    break
        
        if min_height == 20:
            return 1.0
        
        for row in range(min_height, 20):
            for col in range(10):
                total_cells += 1
                if board[row][col] is not None and board[row][col] != 0:
                    filled_cells += 1
        
        return filled_cells / (total_cells + 1)
    
    def calculate_reward(self, prev_state, current_state):
        """í˜•íƒœ ìµœì í™”ì— íŠ¹í™”ëœ ë³´ìƒ (ê· í˜•ì¡íŒ í˜•íƒœ ìœ ë„)"""
        if not prev_state or not current_state:
            return 0
        
        reward = 0
        
        # ê¸°ë³¸ ë³´ìƒ
        score_diff = current_state.get('score', 0) - prev_state.get('score', 0)
        reward += score_diff * 0.01
        
        prev_board = prev_state.get('board', [])
        curr_board = current_state.get('board', [])
        
        prev_heights = self._get_column_heights(prev_board)
        curr_heights = self._get_column_heights(curr_board)
        
        # í˜•íƒœ ê°œì„  ë³´ìƒ (í‘œë©´ ê±°ì¹ ê¸° ê°œì„ )
        prev_roughness = self._calculate_roughness(prev_heights)
        curr_roughness = self._calculate_roughness(curr_heights)
        roughness_improvement = prev_roughness - curr_roughness
        reward += roughness_improvement * 3  # 2 â†’ 3ìœ¼ë¡œ ì¦ê°€
        
        # ì•ˆì •ì„± ê°œì„  ë³´ìƒ
        prev_stability = self._calculate_stability(prev_board)
        curr_stability = self._calculate_stability(curr_board)
        stability_improvement = curr_stability - prev_stability
        reward += stability_improvement * 25
        
        # ë°€ì§‘ë„ ê°œì„  ë³´ìƒ
        prev_compactness = self._calculate_compactness(prev_board)
        curr_compactness = self._calculate_compactness(curr_board)
        compactness_improvement = curr_compactness - prev_compactness
        reward += compactness_improvement * 20
        
        # ê· í˜•ì¡íŒ í˜•íƒœ ë³´ìƒ (ìƒˆë¡œ ì¶”ê°€)
        height_variance = self._calculate_height_variance(curr_heights)
        if height_variance < 10:  # ì ë‹¹í•œ ë†’ì´ ë¶„ì‚°ì¼ ë•Œ ë³´ìƒ
            reward += (10 - height_variance) * 2
        elif height_variance > 20:  # ë„ˆë¬´ ë¶ˆê· ë“±í•˜ë©´ íŒ¨ë„í‹°
            reward -= (height_variance - 20) * 3
        
        # ê·¹ë‹¨ì  í¸ì¤‘ ë°©ì§€ (ìƒˆë¡œ ì¶”ê°€)
        max_height = max(curr_heights) if curr_heights else 0
        min_height = min(curr_heights) if curr_heights else 0
        height_diff = max_height - min_height
        if height_diff > 10:  # 10ì¹¸ ì´ìƒ ì°¨ì´ë‚˜ë©´ íŒ¨ë„í‹°
            reward -= (height_diff - 10) * 5
        
        # ì¢Œìš° ëŒ€ì¹­ì„± ì¥ë ¤ (ìƒˆë¡œ ì¶”ê°€)
        left_avg = sum(curr_heights[:5]) / 5 if curr_heights else 0
        right_avg = sum(curr_heights[5:]) / 5 if curr_heights else 0
        symmetry_diff = abs(left_avg - right_avg)
        if symmetry_diff <= 1:  # ì¢Œìš° ê· í˜•ì´ ì¢‹ìœ¼ë©´ ë³´ìƒ
            reward += 10
        elif symmetry_diff > 5:  # ì¢Œìš° ë¶ˆê· í˜•ì´ ì‹¬í•˜ë©´ íŒ¨ë„í‹°
            reward -= symmetry_diff * 2
        
        # ë¼ì¸ í´ë¦¬ì–´ ë³´ìƒ
        lines_diff = current_state.get('lines', 0) - prev_state.get('lines', 0)
        reward += lines_diff * 15
        
        # ë†’ì´ íŒ¨ë„í‹°
        if max_height > 15:
            reward -= (max_height - 15) * 3
        
        # ê²Œì„ ì˜¤ë²„ íŒ¨ë„í‹°
        if current_state.get('isGameOver', False):
            reward -= 100
        
        return reward

# 3. ìŠ¤íƒ ìµœì í™” ì „ë¬¸ AI  
class StackOptimizingAI(BaseDQNAgent):
    def __init__(self):
        super().__init__(input_size=19, learning_rate=0.001)  # 10 + 3 + 2 + 1 + 3 = 19
        self.name = "StackOptimizing"
    
    def extract_features(self, state):
        """ê· ë“±í•œ ìŠ¤íƒ ë†’ì´ ìœ ì§€ì— íŠ¹í™”ëœ íŠ¹ì§•"""
        if not state or 'board' not in state:
            return np.zeros(self.input_size)
        
        board = state['board']
        features = []
        
        # ë†’ì´ ë¶„í¬
        heights = self._get_column_heights(board)
        features.extend([h / 20.0 for h in heights])  # 10ê°œ
        
        # ë†’ì´ í†µê³„
        mean_height = sum(heights) / len(heights) if heights else 0
        features.append(mean_height / 20.0)
        features.append(self._calculate_height_std(heights) / 10.0)
        features.append((max(heights) - min(heights)) / 20.0 if heights else 0)
        
        # ë†’ì´ ê· ë“±ì„±
        features.append(self._calculate_evenness_score(heights))
        features.append(self._calculate_balance_score(heights))
        
        # ìœ„í—˜ ì§€í‘œ
        features.append(self._calculate_danger_score(heights))
        
        # ê²Œì„ ìƒíƒœ
        features.append(state.get('score', 0) / 10000.0)
        features.append(state.get('lines', 0) / 100.0)
        features.append(state.get('level', 1) / 20.0)
        
        return np.array(features[:self.input_size], dtype=np.float32)
    
    def _get_column_heights(self, board):
        """ê° ì—´ì˜ ë†’ì´"""
        heights = []
        for col in range(10):
            height = 0
            for row in range(20):
                if board[row][col] is not None and board[row][col] != 0:
                    height = 20 - row
                    break
            heights.append(height)
        return heights
    
    def _calculate_height_std(self, heights):
        """ë†’ì´ í‘œì¤€í¸ì°¨"""
        if not heights:
            return 0
        mean = sum(heights) / len(heights)
        variance = sum((h - mean) ** 2 for h in heights) / len(heights)
        return variance ** 0.5
    
    def _calculate_evenness_score(self, heights):
        """ë†’ì´ ê· ë“±ì„± ì ìˆ˜ (0-1, 1ì´ ê°€ì¥ ê· ë“±)"""
        if not heights:
            return 1.0
        
        max_diff = max(heights) - min(heights)
        return max(0, 1 - max_diff / 20.0)
    
    def _calculate_balance_score(self, heights):
        """ì¢Œìš° ê· í˜• ì ìˆ˜"""
        if len(heights) < 10:
            return 0.5
        
        left_avg = sum(heights[:5]) / 5
        right_avg = sum(heights[5:]) / 5
        diff = abs(left_avg - right_avg)
        return max(0, 1 - diff / 10.0)
    
    def _calculate_danger_score(self, heights):
        """ìœ„í—˜ë„ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ìœ„í—˜)"""
        dangerous_heights = sum(1 for h in heights if h > 15)
        return dangerous_heights / 10.0
    
    def calculate_reward(self, prev_state, current_state):
        """ìŠ¤íƒ ìµœì í™”ì— íŠ¹í™”ëœ ë³´ìƒ (í•œìª½ ìŒ“ê¸° ë°©ì§€ ê°•í™”)"""
        if not prev_state or not current_state:
            return 0
        
        reward = 0
        
        # ê¸°ë³¸ ë³´ìƒ
        score_diff = current_state.get('score', 0) - prev_state.get('score', 0)
        reward += score_diff * 0.01
        
        prev_board = prev_state.get('board', [])
        curr_board = current_state.get('board', [])
        
        prev_heights = self._get_column_heights(prev_board)
        curr_heights = self._get_column_heights(curr_board)
        
        # ê· ë“±ì„± ê°œì„  ë³´ìƒ (ê°•í™”)
        prev_evenness = self._calculate_evenness_score(prev_heights)
        curr_evenness = self._calculate_evenness_score(curr_heights)
        evenness_improvement = curr_evenness - prev_evenness
        reward += evenness_improvement * 100  # 50 â†’ 100ìœ¼ë¡œ ì¦ê°€
        
        # ê· í˜• ê°œì„  ë³´ìƒ (ê°•í™”)
        prev_balance = self._calculate_balance_score(prev_heights)
        curr_balance = self._calculate_balance_score(curr_heights)
        balance_improvement = curr_balance - prev_balance
        reward += balance_improvement * 60  # 30 â†’ 60ìœ¼ë¡œ ì¦ê°€
        
        # ìœ„í—˜ë„ ê°ì†Œ ë³´ìƒ
        prev_danger = self._calculate_danger_score(prev_heights)
        curr_danger = self._calculate_danger_score(curr_heights)
        danger_reduction = prev_danger - curr_danger
        reward += danger_reduction * 40
        
        # í•œìª½ í¸ì¤‘ ë°©ì§€ íŒ¨ë„í‹° (ìƒˆë¡œ ì¶”ê°€)
        height_variance = self._calculate_height_std(curr_heights)
        if height_variance > 5:  # ë†’ì´ ì°¨ì´ê°€ í´ ë•Œ íŒ¨ë„í‹°
            reward -= height_variance * 5
        
        # ê·¹ë‹¨ì  ë†’ì´ ì°¨ì´ íŒ¨ë„í‹° (ìƒˆë¡œ ì¶”ê°€)
        max_height = max(curr_heights) if curr_heights else 0
        min_height = min(curr_heights) if curr_heights else 0
        extreme_diff = max_height - min_height
        if extreme_diff > 8:  # 8ì¹¸ ì´ìƒ ì°¨ì´ë‚˜ë©´ ê°•í•œ íŒ¨ë„í‹°
            reward -= (extreme_diff - 8) * 15
        
        # ê°€ìš´ë° ì˜ì—­ ì‚¬ìš© ì¥ë ¤ (ìƒˆë¡œ ì¶”ê°€)
        center_heights = curr_heights[3:7]  # ê°€ìš´ë° 4ì—´
        edge_heights = curr_heights[:3] + curr_heights[7:]  # ì–‘ìª½ ë 6ì—´
        center_avg = sum(center_heights) / len(center_heights) if center_heights else 0
        edge_avg = sum(edge_heights) / len(edge_heights) if edge_heights else 0
        
        # ê°€ìš´ë°ê°€ ë„ˆë¬´ ë†’ìœ¼ë©´ íŒ¨ë„í‹°, ì ì ˆíˆ ì‚¬ìš©í•˜ë©´ ë³´ìƒ
        if center_avg > edge_avg + 2:
            reward -= 10
        elif abs(center_avg - edge_avg) <= 2:
            reward += 5
        
        # ë¼ì¸ í´ë¦¬ì–´ ë³´ìƒ
        lines_diff = current_state.get('lines', 0) - prev_state.get('lines', 0)
        reward += lines_diff * 20
        
        # ìµœëŒ€ ë†’ì´ íŒ¨ë„í‹°
        if max_height > 18:
            reward -= (max_height - 18) * 10
        
        # ê²Œì„ ì˜¤ë²„ íŒ¨ë„í‹°
        if current_state.get('isGameOver', False):
            reward -= 100
        
        return reward

# 4. ë¼ì¸ í´ë¦¬ì–´ ì „ë¬¸ AI
class LineClearingAI(BaseDQNAgent):
    def __init__(self):
        super().__init__(input_size=25, learning_rate=0.001)  # 5 + 5 + 1 + 1 + 10 + 3 = 25
        self.name = "LineClearing"
    
    def extract_features(self, state):
        """ë‚®ì€ ì¸µ ì™„ì„±ì— íŠ¹í™”ëœ íŠ¹ì§•"""
        if not state or 'board' not in state:
            return np.zeros(self.input_size)
        
        board = state['board']
        features = []
        
        # ê° í–‰ì˜ ì±„ì›Œì§„ ì •ë„ (í•˜ë‹¨ë¶€í„°)
        for row in range(15, 20):  # í•˜ë‹¨ 5ì¤„
            filled_count = sum(1 for cell in board[row] if cell is not None and cell != 0)
            features.append(filled_count / 10.0)
        
        # ê° í–‰ì˜ ë¹ˆ ì¹¸ íŒ¨í„´
        for row in range(15, 20):
            empty_positions = []
            for col in range(10):
                if board[row][col] is None or board[row][col] == 0:
                    empty_positions.append(col)
            
            # ë¹ˆ ì¹¸ì˜ ì—°ì†ì„±
            continuity = self._calculate_continuity(empty_positions)
            features.append(continuity)
        
        # ì™„ì„± ê°€ëŠ¥í•œ ë¼ì¸ ìˆ˜
        almost_complete = 0
        for row in range(20):
            filled = sum(1 for cell in board[row] if cell is not None and cell != 0)
            if filled >= 8:
                almost_complete += 1
        features.append(almost_complete / 10.0)
        
        # í•˜ë‹¨ ë°€ì§‘ë„
        bottom_density = self._calculate_bottom_density(board)
        features.append(bottom_density)
        
        # ê° ì—´ì˜ ë†’ì´
        heights = self._get_column_heights(board)
        features.extend([h / 20.0 for h in heights])
        
        # ê²Œì„ ìƒíƒœ
        features.append(state.get('score', 0) / 10000.0)
        features.append(state.get('lines', 0) / 100.0)
        features.append(state.get('level', 1) / 20.0)
        
        return np.array(features[:self.input_size], dtype=np.float32)
    
    def _calculate_continuity(self, empty_positions):
        """ë¹ˆ ì¹¸ì˜ ì—°ì†ì„± ê³„ì‚°"""
        if not empty_positions:
            return 1.0
        if len(empty_positions) == 1:
            return 0.5
        
        consecutive_groups = 0
        current_group_size = 1
        
        for i in range(1, len(empty_positions)):
            if empty_positions[i] == empty_positions[i-1] + 1:
                current_group_size += 1
            else:
                consecutive_groups += 1
                current_group_size = 1
        consecutive_groups += 1
        
        # ê·¸ë£¹ì´ ì ì„ìˆ˜ë¡ ì¢‹ìŒ
        return max(0, 1 - consecutive_groups / 5.0)
    
    def _calculate_bottom_density(self, board):
        """í•˜ë‹¨ 5ì¤„ì˜ ë°€ì§‘ë„"""
        total_cells = 50  # 5í–‰ * 10ì—´
        filled_cells = 0
        
        for row in range(15, 20):
            for col in range(10):
                if board[row][col] is not None and board[row][col] != 0:
                    filled_cells += 1
        
        return filled_cells / total_cells
    
    def _get_column_heights(self, board):
        """ê° ì—´ì˜ ë†’ì´"""
        heights = []
        for col in range(10):
            height = 0
            for row in range(20):
                if board[row][col] is not None and board[row][col] != 0:
                    height = 20 - row
                    break
            heights.append(height)
        return heights
    
    def calculate_reward(self, prev_state, current_state):
        """ë¼ì¸ í´ë¦¬ì–´ì— íŠ¹í™”ëœ ë³´ìƒ"""
        if not prev_state or not current_state:
            return 0
        
        reward = 0
        
        # ê¸°ë³¸ ë³´ìƒ
        score_diff = current_state.get('score', 0) - prev_state.get('score', 0)
        reward += score_diff * 0.02
        
        # ë¼ì¸ í´ë¦¬ì–´ ë³´ìƒ (ë§¤ìš° ë†’ì€ ê°€ì¤‘ì¹˜)
        lines_diff = current_state.get('lines', 0) - prev_state.get('lines', 0)
        reward += lines_diff * 100
        
        prev_board = prev_state.get('board', [])
        curr_board = current_state.get('board', [])
        
        # í•˜ë‹¨ ë°€ì§‘ë„ ê°œì„  ë³´ìƒ
        prev_density = self._calculate_bottom_density(prev_board)
        curr_density = self._calculate_bottom_density(curr_board)
        density_improvement = curr_density - prev_density
        reward += density_improvement * 50
        
        # ê±°ì˜ ì™„ì„±ëœ ë¼ì¸ ìƒì„± ë³´ìƒ
        prev_almost = self._count_almost_complete_lines(prev_board)
        curr_almost = self._count_almost_complete_lines(curr_board)
        almost_improvement = curr_almost - prev_almost
        reward += almost_improvement * 20
        
        # í•˜ë‹¨ í–‰ ì™„ì„±ë„ ê°œì„  ë³´ìƒ
        for row in range(17, 20):  # í•˜ë‹¨ 3ì¤„
            prev_filled = sum(1 for cell in prev_board[row] if cell is not None and cell != 0)
            curr_filled = sum(1 for cell in curr_board[row] if cell is not None and cell != 0)
            row_improvement = curr_filled - prev_filled
            reward += row_improvement * 5 * (21 - row)  # ë” ì•„ë˜ì¼ìˆ˜ë¡ ë†’ì€ ë³´ìƒ
        
        # ê²Œì„ ì˜¤ë²„ íŒ¨ë„í‹°
        if current_state.get('isGameOver', False):
            reward -= 100
        
        return reward
    
    def _count_almost_complete_lines(self, board):
        """ê±°ì˜ ì™„ì„±ëœ ë¼ì¸ ìˆ˜"""
        count = 0
        for row in range(20):
            filled = sum(1 for cell in board[row] if cell is not None and cell != 0)
            if filled >= 8:
                count += 1
        return count

# 5. ì „ëµì  ë©€í‹°ë¼ì¸ í´ë¦¬ì–´ AI
class StrategicAI(BaseDQNAgent):
    def __init__(self):
        super().__init__(input_size=36, learning_rate=0.001)  # 20 + 3 + 1 + 9 + 3 = 36
        self.name = "Strategic"
    
    def extract_features(self, state):
        """ì „ëµì  ë©€í‹°ë¼ì¸ í´ë¦¬ì–´ì— íŠ¹í™”ëœ íŠ¹ì§•"""
        if not state or 'board' not in state:
            return np.zeros(self.input_size)
        
        board = state['board']
        features = []
        
        # ê° í–‰ì˜ ì™„ì„±ë„
        for row in range(20):
            filled = sum(1 for cell in board[row] if cell is not None and cell != 0)
            features.append(filled / 10.0)
        
        # ë©€í‹°ë¼ì¸ ê°€ëŠ¥ì„± ë¶„ì„
        features.append(self._analyze_tetris_potential(board))
        features.append(self._analyze_triple_potential(board))
        features.append(self._analyze_double_potential(board))
        
        # ì›° (ê¹Šì€ êµ¬ë©) ë¶„ì„
        well_depth = self._find_deepest_well(board)
        features.append(well_depth / 20.0)
        
        # ìŠ¤íƒ íŒ¨í„´ ë¶„ì„
        features.extend(self._analyze_stack_pattern(board))
        
        # ê²Œì„ ìƒíƒœ
        features.append(state.get('score', 0) / 10000.0)
        features.append(state.get('lines', 0) / 100.0)
        features.append(state.get('level', 1) / 20.0)
        
        return np.array(features[:self.input_size], dtype=np.float32)
    
    def _analyze_tetris_potential(self, board):
        """í…ŒíŠ¸ë¦¬ìŠ¤ (4ë¼ì¸) ê°€ëŠ¥ì„±"""
        # 4ì¤„ì´ ì—°ì†ìœ¼ë¡œ ê±°ì˜ ì™„ì„±ë˜ì–´ ìˆê³ , í•œ ì—´ì´ ë¹„ì–´ìˆëŠ” íŒ¨í„´
        for start_row in range(17):  # 4ì¤„ í™•ì¸ ê°€ëŠ¥í•œ ì‹œì‘ì 
            potential_cols = []
            for col in range(10):
                col_empty_in_4_rows = True
                for row in range(start_row, start_row + 4):
                    if board[row][col] is not None and board[row][col] != 0:
                        col_empty_in_4_rows = False
                        break
                
                if col_empty_in_4_rows:
                    # ì´ ì—´ì´ 4ì¤„ì—ì„œ ëª¨ë‘ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
                    other_cols_full = True
                    for check_col in range(10):
                        if check_col != col:
                            for row in range(start_row, start_row + 4):
                                if board[row][check_col] is None or board[row][check_col] == 0:
                                    other_cols_full = False
                                    break
                    if other_cols_full:
                        return 1.0
        return 0.0
    
    def _analyze_triple_potential(self, board):
        """3ë¼ì¸ í´ë¦¬ì–´ ê°€ëŠ¥ì„±"""
        potential_count = 0
        for start_row in range(18):  # 3ì¤„ í™•ì¸
            almost_complete = 0
            for row in range(start_row, start_row + 3):
                filled = sum(1 for cell in board[row] if cell is not None and cell != 0)
                if filled >= 9:
                    almost_complete += 1
            if almost_complete >= 2:
                potential_count += 1
        return min(potential_count / 5.0, 1.0)
    
    def _analyze_double_potential(self, board):
        """2ë¼ì¸ í´ë¦¬ì–´ ê°€ëŠ¥ì„±"""
        potential_count = 0
        for start_row in range(19):  # 2ì¤„ í™•ì¸
            almost_complete = 0
            for row in range(start_row, start_row + 2):
                filled = sum(1 for cell in board[row] if cell is not None and cell != 0)
                if filled >= 8:
                    almost_complete += 1
            if almost_complete == 2:
                potential_count += 1
        return min(potential_count / 10.0, 1.0)
    
    def _find_deepest_well(self, board):
        """ê°€ì¥ ê¹Šì€ ì›° ì°¾ê¸°"""
        heights = self._get_column_heights(board)
        max_well_depth = 0
        
        for col in range(10):
            # ì–‘ìª½ ì´ì›ƒë³´ë‹¤ ë‚®ì€ ì—´ì´ ì›°
            left_height = heights[col-1] if col > 0 else 0
            right_height = heights[col+1] if col < 9 else 0
            current_height = heights[col]
            
            well_depth = min(left_height, right_height) - current_height
            if well_depth > 0:
                max_well_depth = max(max_well_depth, well_depth)
        
        return max_well_depth
    
    def _analyze_stack_pattern(self, board):
        """ìŠ¤íƒ íŒ¨í„´ ë¶„ì„"""
        heights = self._get_column_heights(board)
        patterns = []
        
        # ê²½ì‚¬ íŒ¨í„´ (í…ŒíŠ¸ë¦¬ìŠ¤ ì¤€ë¹„ìš©)
        for i in range(7):  # 4ì—´ ì—°ì† í™•ì¸
            slope_pattern = all(heights[i+j] >= heights[i+j+1] for j in range(3))
            patterns.append(1.0 if slope_pattern else 0.0)
        
        # ì•ˆì •ì„± íŒ¨í„´
        stable_pattern = all(abs(heights[i] - heights[i+1]) <= 2 for i in range(9))
        patterns.append(1.0 if stable_pattern else 0.0)
        
        # ì›° íŒ¨í„´ (í•œìª½ì´ ê¹Šê²Œ íŒŒì¸ íŒ¨í„´)
        well_pattern = any(heights[i] < heights[i-1] - 3 and heights[i] < heights[i+1] - 3 
                          for i in range(1, 9))
        patterns.append(1.0 if well_pattern else 0.0)
        
        return patterns
    
    def _get_column_heights(self, board):
        """ê° ì—´ì˜ ë†’ì´"""
        heights = []
        for col in range(10):
            height = 0
            for row in range(20):
                if board[row][col] is not None and board[row][col] != 0:
                    height = 20 - row
                    break
            heights.append(height)
        return heights
    
    def calculate_reward(self, prev_state, current_state):
        """ì „ëµì  í”Œë ˆì´ì— íŠ¹í™”ëœ ë³´ìƒ"""
        if not prev_state or not current_state:
            return 0
        
        reward = 0
        
        # ê¸°ë³¸ ë³´ìƒ
        score_diff = current_state.get('score', 0) - prev_state.get('score', 0)
        reward += score_diff * 0.02
        
        # ë©€í‹°ë¼ì¸ í´ë¦¬ì–´ ë³´ìƒ (ê¸°í•˜ê¸‰ìˆ˜ì  ì¦ê°€)
        lines_diff = current_state.get('lines', 0) - prev_state.get('lines', 0)
        if lines_diff == 4:  # í…ŒíŠ¸ë¦¬ìŠ¤
            reward += 1000
        elif lines_diff == 3:  # íŠ¸ë¦¬í”Œ
            reward += 300
        elif lines_diff == 2:  # ë”ë¸”
            reward += 100
        elif lines_diff == 1:  # ì‹±ê¸€
            reward += 25
        
        prev_board = prev_state.get('board', [])
        curr_board = current_state.get('board', [])
        
        # í…ŒíŠ¸ë¦¬ìŠ¤ ì¤€ë¹„ ë³´ìƒ
        prev_tetris_potential = self._analyze_tetris_potential(prev_board)
        curr_tetris_potential = self._analyze_tetris_potential(curr_board)
        tetris_setup_improvement = curr_tetris_potential - prev_tetris_potential
        reward += tetris_setup_improvement * 200
        
        # ì „ëµì  íŒ¨í„´ ë³´ìƒ
        prev_patterns = self._analyze_stack_pattern(prev_board)
        curr_patterns = self._analyze_stack_pattern(curr_board)
        pattern_improvement = sum(curr_patterns) - sum(prev_patterns)
        reward += pattern_improvement * 50
        
        # ì›° ê´€ë¦¬ ë³´ìƒ
        prev_well = self._find_deepest_well(prev_board)
        curr_well = self._find_deepest_well(curr_board)
        if curr_well > prev_well and curr_well >= 4:  # ê¹Šì€ ì›° ìƒì„±
            reward += 100
        elif curr_well < prev_well and prev_well >= 4:  # ì›° ì‚¬ìš©
            reward += 150
        
        # ê²Œì„ ì˜¤ë²„ íŒ¨ë„í‹°
        if current_state.get('isGameOver', False):
            reward -= 150
        
        return reward

# API í´ë¼ì´ì–¸íŠ¸
class TetrisAPIClient:
    def __init__(self, base_url="http://localhost:3000"):
        self.base_url = base_url
        self.api_url = f"{base_url}/api/game"
    
    def get_game_state(self):
        try:
            response = requests.get(self.api_url, timeout=5)
            if response.status_code == 200:
                data = response.json()
                return data.get('data', {})
        except:
            pass
        return None
    
    def send_action(self, action):
        try:
            response = requests.post(self.api_url, json={
                'type': 'action', 
                'action': action
            }, timeout=5)
            return response.status_code == 200
        except:
            return False

# ë©€í‹° AI ì‹œìŠ¤í…œ
class MultiAITetrisSystem:
    def __init__(self):
        if not PYTORCH_AVAILABLE:
            print("âŒ PyTorchê°€ í•„ìš”í•©ë‹ˆë‹¤. 'pip install torch' ì‹¤í–‰ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
            return
        
        self.client = TetrisAPIClient()
        
        # 5ê°œì˜ ì „ë¬¸í™”ëœ AI ì´ˆê¸°í™”
        self.ais = {
            'hole_finding': HoleFindingAI(),
            'shape_optimizing': ShapeOptimizingAI(), 
            'stack_optimizing': StackOptimizingAI(),
            'line_clearing': LineClearingAI(),
            'strategic': StrategicAI()
        }
        
        # ëª¨ë¸ ë¡œë“œ
        for name, ai in self.ais.items():
            model_path = f"models/{name}_model.pth"
            os.makedirs("models", exist_ok=True)
            ai.load_model(model_path)
        
        print(f"âœ… {len(self.ais)}ê°œì˜ ì „ë¬¸í™”ëœ AI ë¡œë“œ ì™„ë£Œ")
    
    def train_all(self, episodes=50, max_steps=500):
        """ëª¨ë“  AI í›ˆë ¨"""
        print("ğŸ§  ë‹¤ì¤‘ AI í›ˆë ¨ ì‹œì‘!")
        print(f"ğŸ“Š ê° AIë‹¹ {episodes} ì—í”¼ì†Œë“œ, ìµœëŒ€ {max_steps} ìŠ¤í…")
        print("=" * 60)
        
        for ai_name, ai in self.ais.items():
            print(f"\nğŸ¤– {ai_name.upper()} AI í›ˆë ¨ ì‹œì‘")
            print("-" * 40)
            self._train_single_ai(ai, ai_name, episodes, max_steps)
            
            # ëª¨ë¸ ì €ì¥
            model_path = f"models/{ai_name}_model.pth"
            ai.save_model(model_path)
            print(f"ğŸ’¾ {ai_name} ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
            
            # AI ê°„ íœ´ì‹
            print("â³ ë‹¤ìŒ AI í›ˆë ¨ê¹Œì§€ 5ì´ˆ ëŒ€ê¸°...")
            time.sleep(5)
        
        print("\nâœ… ëª¨ë“  AI í›ˆë ¨ ì™„ë£Œ!")
    
    def _train_single_ai(self, ai, ai_name, episodes, max_steps):
        """ë‹¨ì¼ AI í›ˆë ¨"""
        for episode in range(episodes):
            print(f"ğŸ“ˆ ì—í”¼ì†Œë“œ {episode + 1}/{episodes}")
            
            total_reward = 0
            step = 0
            
            # ê²Œì„ ì´ˆê¸°í™”
            prev_state = self.client.get_game_state()
            if not prev_state:
                print("âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨")
                continue
            
            for step in range(max_steps):
                current_state = self.client.get_game_state()
                if not current_state:
                    break
                
                # ê²Œì„ ì˜¤ë²„ ì²˜ë¦¬ (ê°•í™”ëœ ê°ì§€)
                is_game_over = current_state.get('isGameOver', False)
                
                # ì¶”ê°€ì ì¸ ê²Œì„ ì˜¤ë²„ ê°ì§€ ì¡°ê±´ë“¤
                if not is_game_over:
                    board = current_state.get('board', [])
                    if board:
                        # ìƒë‹¨ 3ì¤„ì— ë¸”ë¡ì´ ìˆëŠ”ì§€ í™•ì¸
                        for row in range(3):
                            if any(board[row][col] is not None and board[row][col] != 0 for col in range(10)):
                                is_game_over = True
                                print(f"   ğŸš¨ AIê°€ ê²Œì„ ì˜¤ë²„ ê°ì§€: ìƒë‹¨ ì˜ì—­ ë¸”ë¡ ë°œê²¬ (í–‰ {row})")
                                break
                
                if is_game_over:
                    print(f"   ğŸ’€ ê²Œì„ ì˜¤ë²„ ê°ì§€! ì ìˆ˜: {current_state.get('score', 0)}, ë¼ì¸: {current_state.get('lines', 0)}")
                    
                    print(f"   ğŸ”„ ì¦‰ì‹œ ì¬ì‹œì‘ ì‹œë„...")
                    if self.client.send_action('restart'):
                        print(f"   âœ… ì¬ì‹œì‘ ìš”ì²­ ì „ì†¡ë¨")
                        time.sleep(1.5)  # ì¬ì‹œì‘ ì²˜ë¦¬ ëŒ€ê¸°
                        
                        # ì¬ì‹œì‘ ì„±ê³µ í™•ì¸ (ì—¬ëŸ¬ ë²ˆ ì‹œë„)
                        for attempt in range(3):
                            time.sleep(0.5)
                            new_state = self.client.get_game_state()
                            if new_state and not new_state.get('isGameOver', False) and new_state.get('score', 0) == 0:
                                print(f"   ğŸ® ìƒˆ ê²Œì„ ì‹œì‘ í™•ì¸ (ì‹œë„ {attempt + 1})")
                                break
                        else:
                            print(f"   âš ï¸ ì¬ì‹œì‘ í™•ì¸ ì‹¤íŒ¨, ë‹¤ìŒ ì—í”¼ì†Œë“œë¡œ...")
                            break
                        continue
                    else:
                        print(f"   âŒ ì¬ì‹œì‘ ìš”ì²­ ì‹¤íŒ¨")
                        break
                
                # AI ì•¡ì…˜ ì„ íƒ ë° ì‹¤í–‰
                action_idx = ai.get_action(current_state)
                action = ai.actions[action_idx]
                
                if not self.client.send_action(action):
                    continue
                
                time.sleep(0.3)
                
                # ë‹¤ìŒ ìƒíƒœ ë° ë³´ìƒ
                next_state = self.client.get_game_state()
                if not next_state:
                    continue
                
                reward = ai.calculate_reward(current_state, next_state)
                total_reward += reward
                
                # ê²½í—˜ ì €ì¥
                done = next_state.get('isGameOver', False)
                ai.remember(current_state, action_idx, reward, next_state, done)
                
                # í•™ìŠµ
                ai.replay()
                
                # ì§„í–‰ ìƒí™©
                if step % 50 == 0:
                    score = next_state.get('score', 0)
                    lines = next_state.get('lines', 0)
                    print(f"   ğŸ“Š ìŠ¤í… {step}: ì ìˆ˜ {score}, ë¼ì¸ {lines}, ë³´ìƒ {reward:.2f}")
                
                prev_state = current_state
            
            # ì—í”¼ì†Œë“œ ê²°ê³¼
            final_state = self.client.get_game_state()
            final_score = final_state.get('score', 0) if final_state else 0
            final_lines = final_state.get('lines', 0) if final_state else 0
            
            print(f"   ğŸ¯ ì™„ë£Œ: ì ìˆ˜ {final_score}, ë¼ì¸ {final_lines}, ì´ë³´ìƒ {total_reward:.2f}")
            print(f"   ğŸ” íƒí—˜ë¥ : {ai.epsilon:.3f}")
    
    def play_with_ai(self, ai_name, duration=120):
        """íŠ¹ì • AIë¡œ í”Œë ˆì´"""
        if ai_name not in self.ais:
            print(f"âŒ AI '{ai_name}' ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥: {list(self.ais.keys())}")
            return
        
        ai = self.ais[ai_name]
        print(f"ğŸ¤– {ai_name.upper()} AI í”Œë ˆì´ ì‹œì‘!")
        print(f"ğŸ• í”Œë ˆì´ ì‹œê°„: {duration}ì´ˆ")
        print("=" * 40)
        
        start_time = time.time()
        step = 0
        
        while time.time() - start_time < duration:
            state = self.client.get_game_state()
            if not state:
                time.sleep(0.5)
                continue
            
            # ê²Œì„ ì˜¤ë²„ ì²˜ë¦¬ (ê°•í™”ëœ ê°ì§€)
            is_game_over = state.get('isGameOver', False)
            
            # ì¶”ê°€ì ì¸ ê²Œì„ ì˜¤ë²„ ê°ì§€ ì¡°ê±´ë“¤
            if not is_game_over:
                board = state.get('board', [])
                if board:
                    # ìƒë‹¨ 3ì¤„ì— ë¸”ë¡ì´ ìˆëŠ”ì§€ í™•ì¸
                    for row in range(3):
                        if any(board[row][col] is not None and board[row][col] != 0 for col in range(10)):
                            is_game_over = True
                            print(f"ğŸš¨ AIê°€ ê²Œì„ ì˜¤ë²„ ê°ì§€: ìƒë‹¨ ì˜ì—­ ë¸”ë¡ ë°œê²¬ (í–‰ {row})")
                            break
            
            if is_game_over:
                print(f"ğŸ¯ ê²Œì„ ì˜¤ë²„ ê°ì§€! ì ìˆ˜: {state.get('score', 0)}, ë¼ì¸: {state.get('lines', 0)}")
                
                print(f"ğŸ”„ ì¦‰ì‹œ ì¬ì‹œì‘ ì‹œë„...")
                if self.client.send_action('restart'):
                    print(f"âœ… ì¬ì‹œì‘ ìš”ì²­ ì „ì†¡ë¨")
                    time.sleep(1.5)  # ì¬ì‹œì‘ ì²˜ë¦¬ ëŒ€ê¸°
                    
                    # ì¬ì‹œì‘ ì„±ê³µ í™•ì¸ (ì—¬ëŸ¬ ë²ˆ ì‹œë„)
                    for attempt in range(3):
                        time.sleep(0.5)
                        new_state = self.client.get_game_state()
                        if new_state and not new_state.get('isGameOver', False) and new_state.get('score', 0) == 0:
                            print(f"ğŸ® ìƒˆ ê²Œì„ ì‹œì‘ í™•ì¸ (ì‹œë„ {attempt + 1}), ê³„ì† í”Œë ˆì´...")
                            break
                    else:
                        print(f"âš ï¸ ì¬ì‹œì‘ í™•ì¸ ì‹¤íŒ¨")
                        break
                    continue
                else:
                    print(f"âŒ ì¬ì‹œì‘ ìš”ì²­ ì‹¤íŒ¨")
                    break
            
            # ìµœì  ì•¡ì…˜ ì„ íƒ (íƒí—˜ ì—†ì´)
            old_epsilon = ai.epsilon
            ai.epsilon = 0
            action_idx = ai.get_action(state)
            action = ai.actions[action_idx]
            ai.epsilon = old_epsilon
            
            if self.client.send_action(action):
                step += 1
                if step % 20 == 0:
                    score = state.get('score', 0)
                    lines = state.get('lines', 0)
                    print(f"ğŸ® ìŠ¤í… {step}: ì ìˆ˜ {score}, ë¼ì¸ {lines}, ì•¡ì…˜ {action}")
            
            time.sleep(0.4)
        
        print(f"âœ… í”Œë ˆì´ ì™„ë£Œ! ì´ {step} ì•¡ì…˜")
    
    def analyze_game_situation(self, state):
        """ê²Œì„ ìƒí™© ë¶„ì„ (ìœ„í—˜ë„, ê³µê°„ í™œìš©ë„, í˜•íƒœ ì í•©ì„±)"""
        if not state or 'board' not in state:
            return {'danger_level': 0, 'space_utilization': 0, 'shape_fitness': 0}
        
        board = state['board']
        analysis = {}
        
        # 1. ìœ„í—˜ë„ í‰ê°€ (0-10, ë†’ì„ìˆ˜ë¡ ìœ„í—˜)
        heights = []
        for col in range(10):
            height = 0
            for row in range(20):
                if board[row][col] is not None and board[row][col] != 0:
                    height = 20 - row
                    break
            heights.append(height)
        
        max_height = max(heights) if heights else 0
        height_variance = sum((h - sum(heights)/len(heights))**2 for h in heights) / len(heights) if heights else 0
        
        danger_level = 0
        if max_height > 15:  # ìƒë‹¨ ì ‘ê·¼
            danger_level += (max_height - 15) * 2
        if height_variance > 25:  # ë¶ˆê· ë“±í•œ ë†’ì´
            danger_level += height_variance * 0.2
        if max_height > 18:  # ë§¤ìš° ìœ„í—˜
            danger_level += 5
        
        analysis['danger_level'] = min(danger_level, 10)
        
        # 2. ê³µê°„ í™œìš©ë„ í‰ê°€ (0-10, ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        # ë„“ì€ ê³µê°„ì´ ìˆëŠ”ì§€ í™•ì¸
        wide_spaces = 0
        for col in range(10):
            if heights[col] < max_height - 3:  # ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ ê³µê°„
                wide_spaces += 1
        
        # êµ¬ë© ê°œìˆ˜ (ì ì„ìˆ˜ë¡ ì¢‹ìŒ)
        holes = 0
        for col in range(10):
            block_found = False
            for row in range(20):
                if board[row][col] is not None and board[row][col] != 0:
                    block_found = True
                elif block_found and (board[row][col] is None or board[row][col] == 0):
                    holes += 1
        
        space_score = wide_spaces * 2 - holes * 0.5
        analysis['space_utilization'] = max(0, min(space_score, 10))
        
        # 3. í˜•íƒœ ì í•©ì„± í‰ê°€ (0-10, ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        # í‘œë©´ í‰íƒ„ë„
        roughness = sum(abs(heights[i] - heights[i+1]) for i in range(9))
        smoothness = max(0, 10 - roughness * 0.5)
        
        # ì¢Œìš° ê· í˜•
        left_avg = sum(heights[:5]) / 5
        right_avg = sum(heights[5:]) / 5
        balance = max(0, 10 - abs(left_avg - right_avg) * 2)
        
        analysis['shape_fitness'] = (smoothness + balance) / 2
        
        return analysis
    
    def analyze_current_piece_fit(self, state):
        """í˜„ì¬ ë¸”ë¡ì´ ë³´ë“œì— ì–¼ë§ˆë‚˜ ì˜ ë§ëŠ”ì§€ ë¶„ì„"""
        current_piece = state.get('currentPiece', {})
        if not current_piece:
            return {'fit_score': 5.0, 'best_position': None}
        
        board = state.get('board', [])
        if not board:
            return {'fit_score': 5.0, 'best_position': None}
        
        # í˜„ì¬ ë¸”ë¡ì˜ í˜•íƒœì™€ ìœ„ì¹˜ ë¶„ì„
        piece_type = current_piece.get('type', '')
        piece_rotation = current_piece.get('rotation', 0)
        
        # ê° ìœ„ì¹˜ì—ì„œì˜ ì í•©ë„ ê³„ì‚°
        best_fit = 0
        best_position = None
        
        for col in range(8):  # ê°€ëŠ¥í•œ ì—´ ìœ„ì¹˜
            fit_score = self._calculate_piece_fit(board, piece_type, col, piece_rotation)
            if fit_score > best_fit:
                best_fit = fit_score
                best_position = col
        
        return {'fit_score': best_fit, 'best_position': best_position}
    
    def _calculate_piece_fit(self, board, piece_type, col, rotation):
        """íŠ¹ì • ìœ„ì¹˜ì—ì„œ ë¸”ë¡ì˜ ì í•©ë„ ê³„ì‚°"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì í•©ë„ ê³„ì‚°
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ê³„ì‚°ì´ í•„ìš”
        
        # ì—´ ë†’ì´ í™•ì¸
        heights = []
        for c in range(10):
            height = 0
            for row in range(20):
                if board[row][c] is not None and board[row][c] != 0:
                    height = 20 - row
                    break
            heights.append(height)
        
        # í•´ë‹¹ ìœ„ì¹˜ì˜ ë†’ì´
        target_height = heights[col] if col < len(heights) else 0
        
        # ì í•©ë„ ì ìˆ˜ ê³„ì‚°
        fit_score = 5.0  # ê¸°ë³¸ ì ìˆ˜
        
        # ë„ˆë¬´ ë†’ì€ ìœ„ì¹˜ëŠ” ê°ì 
        if target_height > 15:
            fit_score -= (target_height - 15) * 0.5
        
        # ì£¼ë³€ ë†’ì´ì™€ì˜ ì¡°í™”
        if col > 0 and col < 9:
            left_height = heights[col - 1]
            right_height = heights[col + 1]
            height_diff = abs(left_height - target_height) + abs(right_height - target_height)
            fit_score -= height_diff * 0.1
        
        return max(0, fit_score)
    
    def get_ai_priority(self, ai_name, situation, piece_analysis=None):
        """ìƒí™©ê³¼ í˜„ì¬ ë¸”ë¡ì„ ê³ ë ¤í•œ AI ìš°ì„ ìˆœìœ„ ê²°ì •"""
        danger = situation['danger_level']
        space = situation['space_utilization']
        shape = situation['shape_fitness']
        
        priorities = {
            'hole_finding': 1.0,      # ê¸°ë³¸ ìš°ì„ ìˆœìœ„
            'shape_optimizing': 1.0,
            'stack_optimizing': 1.0,
            'line_clearing': 1.0,
            'strategic': 1.0
        }
        
        # í˜„ì¬ ë¸”ë¡ ì í•©ë„ ê³ ë ¤
        if piece_analysis and piece_analysis['fit_score'] < 3:
            # ë¸”ë¡ì´ ì˜ ë§ì§€ ì•ŠëŠ” ê²½ìš° í˜•íƒœ ìµœì í™” ìš°ì„ 
            priorities['shape_optimizing'] *= 1.5
            priorities['stack_optimizing'] *= 1.3
        
        # ìœ„í—˜ ìƒí™© (danger > 6): ìƒì¡´ ì „ëµ ìš°ì„ 
        if danger > 6:
            priorities['stack_optimizing'] = 3.0  # ê· ë“±í•œ ë†’ì´ ìœ ì§€ ìµœìš°ì„ 
            priorities['hole_finding'] = 2.5      # êµ¬ë© ë©”ìš°ê¸° ìš°ì„ 
            priorities['line_clearing'] = 2.0     # ë¼ì¸ í´ë¦¬ì–´ ìš°ì„ 
            priorities['shape_optimizing'] = 1.5  # í˜•íƒœ ì •ë¦¬
            priorities['strategic'] = 0.5         # ì „ëµì  í”Œë ˆì´ í›„ìˆœìœ„
            
            # ê·¹ë„ë¡œ ìœ„í—˜í•œ ìƒí™©ì—ì„œëŠ” ë” ë³´ìˆ˜ì ìœ¼ë¡œ
            if danger > 8:
                priorities['stack_optimizing'] = 4.0
                priorities['strategic'] = 0.2
        
        # ì¤‘ê°„ ìœ„í—˜ (3 < danger <= 6): ê· í˜• ì¡íŒ ì „ëµ
        elif danger > 3:
            priorities['stack_optimizing'] = 2.0
            priorities['shape_optimizing'] = 1.8
            priorities['hole_finding'] = 1.5
            priorities['line_clearing'] = 1.3
            priorities['strategic'] = 1.0
        
        # ì•ˆì „ ìƒí™© (danger <= 3): ê³µê°„ í™œìš©ë„ì— ë”°ë¥¸ ì „ëµ
        else:
            if space > 7:  # ë„“ì€ ê³µê°„ ë§ìŒ
                priorities['strategic'] = 2.5     # ì „ëµì  í”Œë ˆì´ ìš°ì„ 
                priorities['line_clearing'] = 2.0 # ë¼ì¸ í´ë¦¬ì–´ ì¤€ë¹„
                priorities['shape_optimizing'] = 1.5
                priorities['stack_optimizing'] = 1.2
                priorities['hole_finding'] = 1.0
            elif space < 4:  # ê³µê°„ ë¶€ì¡±
                priorities['hole_finding'] = 2.0  # êµ¬ë© ë©”ìš°ê¸° ìš°ì„ 
                priorities['shape_optimizing'] = 1.8
                priorities['stack_optimizing'] = 1.5
                priorities['line_clearing'] = 1.2
                priorities['strategic'] = 1.0
            else:  # ë³´í†µ ê³µê°„
                if shape < 5:  # í˜•íƒœ ë¶ˆëŸ‰
                    priorities['shape_optimizing'] = 2.0
                    priorities['stack_optimizing'] = 1.5
                else:  # í˜•íƒœ ì–‘í˜¸
                    priorities['strategic'] = 1.8
                    priorities['line_clearing'] = 1.5
        
        return priorities[ai_name]
    
    def ensemble_action(self, state):
        """ìš°ì„ ìˆœìœ„ ê¸°ë°˜ AI ì„ íƒ ì‹œìŠ¤í…œ"""
        ai_evaluations = {}
        
        # ê²Œì„ ìƒí™© ë¶„ì„
        situation = self.analyze_game_situation(state)
        
        # í˜„ì¬ ë¸”ë¡ ì í•©ë„ ë¶„ì„
        piece_analysis = self.analyze_current_piece_fit(state)
        
        for ai_name, ai in self.ais.items():
            try:
                # íƒí—˜ ì—†ì´ í‰ê°€
                old_epsilon = ai.epsilon
                ai.epsilon = 0
                
                # Q-value ê³„ì‚°
                best_q_value = ai.get_best_q_value(state)
                action_idx = ai.get_action(state)
                action = ai.actions[action_idx]
                
                # ìš°ì„ ìˆœìœ„ ì ìš© (í˜„ì¬ ë¸”ë¡ ë¶„ì„ í¬í•¨)
                priority = self.get_ai_priority(ai_name, situation, piece_analysis)
                
                # ìœ„í—˜ ìƒí™©ì—ì„œëŠ” ìƒì¡´ ì „ëµ ê°•í™”
                if situation['danger_level'] > 6:
                    # ìœ„í—˜í•œ ì•¡ì…˜ íŒ¨ë„í‹° (drop, rotateëŠ” ì‹ ì¤‘í•˜ê²Œ)
                    if action in ['drop'] and situation['space_utilization'] < 5:
                        priority *= 0.5  # ê³µê°„ ë¶€ì¡± ì‹œ drop íŒ¨ë„í‹°
                    elif action in ['rotate'] and situation['shape_fitness'] < 3:
                        priority *= 0.7  # í˜•íƒœ ë¶ˆëŸ‰ ì‹œ rotate íŒ¨ë„í‹°
                    
                    # ë„“ì€ ê³µê°„ í™œìš© ìš°ì„  (ì‚¬ìš©ì ìš”ì²­ ë°˜ì˜)
                    if situation['space_utilization'] > 6:
                        if action in ['left', 'right']:
                            priority *= 1.5  # ë„“ì€ ê³µê°„ìœ¼ë¡œ ì´ë™ ì¥ë ¤
                        elif action in ['drop'] and ai_name == 'stack_optimizing':
                            priority *= 0.8  # ë„“ì€ ê³µê°„ì´ ìˆì„ ë•Œ ê¸‰í•˜ê²Œ ë–¨ì–´ëœ¨ë¦¬ì§€ ì•Šê¸°
                
                # ì¤‘ê°„ ìœ„í—˜ ìƒí™©ì—ì„œë„ ê³µê°„ í™œìš© ê³ ë ¤
                elif situation['danger_level'] > 3:
                    if situation['space_utilization'] > 7:
                        if action in ['left', 'right']:
                            priority *= 1.3  # ê³µê°„ í™œìš© ìœ„ì¹˜ ì¡°ì • ì¥ë ¤
                        elif action in ['rotate'] and piece_analysis['fit_score'] > 5:
                            priority *= 1.2  # ì¢‹ì€ ê³µê°„ì—ì„œ ë§ì¶¤ íšŒì „ ì¥ë ¤
                
                # í˜„ì¬ ë¸”ë¡ ì í•©ë„ì— ë”°ë¥¸ ì¶”ê°€ ì¡°ì •
                if piece_analysis['fit_score'] < 2:
                    # ë¸”ë¡ì´ ë§¤ìš° ì˜ ë§ì§€ ì•ŠëŠ” ê²½ìš°
                    if action in ['left', 'right']:
                        priority *= 1.2  # ìœ„ì¹˜ ì¡°ì • ì•¡ì…˜ ìš°ì„ 
                    elif action in ['drop']:
                        priority *= 0.3  # ê¸‰í•˜ê²Œ ë–¨ì–´ëœ¨ë¦¬ê¸° ì§€ì–‘
                elif piece_analysis['fit_score'] > 7:
                    # ë¸”ë¡ì´ ì˜ ë§ëŠ” ê²½ìš°
                    if action in ['drop']:
                        priority *= 1.3  # ë¹ ë¥´ê²Œ ë†“ê¸° ì¥ë ¤
                
                # ìµœì¢… ì ìˆ˜ = Q-value * ìš°ì„ ìˆœìœ„
                final_score = best_q_value * priority
                
                ai_evaluations[ai_name] = {
                    'q_value': best_q_value,
                    'priority': priority,
                    'final_score': final_score,
                    'action': action,
                    'action_idx': action_idx
                }
                
                # ì›ë˜ epsilon ë³µì›
                ai.epsilon = old_epsilon
                
            except Exception as e:
                print(f"âš ï¸ {ai_name} í‰ê°€ ì‹¤íŒ¨: {e}")
                continue
        
        if not ai_evaluations:
            return None, None, None, None
        
        # ìµœê³  ìµœì¢… ì ìˆ˜ë¥¼ ê°€ì§„ AI ì„ íƒ
        best_ai_name = max(ai_evaluations.keys(), key=lambda name: ai_evaluations[name]['final_score'])
        best_evaluation = ai_evaluations[best_ai_name]
        
        return best_ai_name, best_evaluation['action'], ai_evaluations, situation
    
    def play_with_ensemble(self, duration=120):
        """ìŠ¤ë§ˆíŠ¸ ì•™ìƒë¸” ë°©ì‹ìœ¼ë¡œ í”Œë ˆì´ (ìš°ì„ ìˆœìœ„ ê¸°ë°˜ AI ì„ íƒ)"""
        print("ğŸ§  ìŠ¤ë§ˆíŠ¸ ì•™ìƒë¸” AI í”Œë ˆì´ ì‹œì‘!")
        print("ğŸ“Š ìƒí™©ë³„ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ AI ì„ íƒ ì‹œìŠ¤í…œ")
        print("ğŸ¯ ìƒì¡´ ìš°ì„  ì „ëµ: ìœ„í—˜ ìƒí™©ì—ì„œ ìƒì¡´ ì•¡ì…˜ ìš°ì„  ì„ íƒ")
        print("ğŸ” ê³µê°„ í™œìš© ìµœì í™”: ë„“ì€ ê³µê°„ í™œìš© ìš°ì„ , í˜•íƒœ ë§ì¶¤ íšŒì „ ì¥ë ¤")
        print(f"ğŸ• í”Œë ˆì´ ì‹œê°„: {duration}ì´ˆ")
        print("=" * 60)
        
        start_time = time.time()
        step = 0
        ai_selection_count = {name: 0 for name in self.ais.keys()}
        
        while time.time() - start_time < duration:
            state = self.client.get_game_state()
            if not state:
                time.sleep(0.5)
                continue
            
            # ê²Œì„ ì˜¤ë²„ ì²˜ë¦¬ (ê°•í™”ëœ ê°ì§€)
            is_game_over = state.get('isGameOver', False)
            
            # ì¶”ê°€ì ì¸ ê²Œì„ ì˜¤ë²„ ê°ì§€ ì¡°ê±´ë“¤
            if not is_game_over:
                board = state.get('board', [])
                if board:
                    # ìƒë‹¨ 3ì¤„ì— ë¸”ë¡ì´ ìˆëŠ”ì§€ í™•ì¸
                    for row in range(3):
                        if any(board[row][col] is not None and board[row][col] != 0 for col in range(10)):
                            is_game_over = True
                            print(f"ğŸš¨ ì•™ìƒë¸” AIê°€ ê²Œì„ ì˜¤ë²„ ê°ì§€: ìƒë‹¨ ì˜ì—­ ë¸”ë¡ ë°œê²¬ (í–‰ {row})")
                            break
            
            if is_game_over:
                print(f"ğŸ’€ ê²Œì„ ì˜¤ë²„ ê°ì§€! ì ìˆ˜: {state.get('score', 0)}, ë¼ì¸: {state.get('lines', 0)}")
                
                print(f"ğŸ”„ ì¦‰ì‹œ ì¬ì‹œì‘ ì‹œë„...")
                if self.client.send_action('restart'):
                    print(f"âœ… ì¬ì‹œì‘ ìš”ì²­ ì „ì†¡ë¨")
                    time.sleep(1.5)  # ì¬ì‹œì‘ ì²˜ë¦¬ ëŒ€ê¸°
                    
                    # ì¬ì‹œì‘ ì„±ê³µ í™•ì¸ (ì—¬ëŸ¬ ë²ˆ ì‹œë„)
                    for attempt in range(3):
                        time.sleep(0.5)
                        new_state = self.client.get_game_state()
                        if new_state and not new_state.get('isGameOver', False) and new_state.get('score', 0) == 0:
                            print(f"ğŸ® ìƒˆ ê²Œì„ ì‹œì‘ í™•ì¸ (ì‹œë„ {attempt + 1}), ê³„ì† í”Œë ˆì´...")
                            break
                    else:
                        print(f"âš ï¸ ì¬ì‹œì‘ í™•ì¸ ì‹¤íŒ¨")
                        break
                    continue
                else:
                    print(f"âŒ ì¬ì‹œì‘ ìš”ì²­ ì‹¤íŒ¨")
                    break
            
            # ì•™ìƒë¸” ì•¡ì…˜ ì„ íƒ
            best_ai_name, best_action, all_evaluations, situation = self.ensemble_action(state)
            
            if best_ai_name is None or all_evaluations is None or situation is None:
                print("âš ï¸ ì•™ìƒë¸” í‰ê°€ ì‹¤íŒ¨")
                continue
            
            # ì„ íƒëœ AI ì¹´ìš´íŠ¸ ì¦ê°€
            ai_selection_count[best_ai_name] += 1
            
            if self.client.send_action(best_action):
                step += 1
                if step % 10 == 0:
                    score = state.get('score', 0)
                    lines = state.get('lines', 0)
                    
                    print(f"ğŸ¯ ìŠ¤í… {step}: ì ìˆ˜ {score}, ë¼ì¸ {lines}")
                    print(f"   ğŸ† ì„ íƒëœ AI: {best_ai_name.upper()} (ìµœì¢…ì ìˆ˜: {all_evaluations[best_ai_name]['final_score']:.3f})")
                    
                    # ìƒí™© ë¶„ì„ í‘œì‹œ
                    print(f"   ğŸ“Š ìƒí™© ë¶„ì„: ìœ„í—˜ë„={situation['danger_level']:.1f}, ê³µê°„={situation['space_utilization']:.1f}, í˜•íƒœ={situation['shape_fitness']:.1f}")
                    
                    # ëª¨ë“  AIì˜ í‰ê°€ í‘œì‹œ
                    print("   ğŸ¤– AI í‰ê°€ (Q-value Ã— ìš°ì„ ìˆœìœ„ = ìµœì¢…ì ìˆ˜):")
                    for ai_name, eval_data in sorted(all_evaluations.items(), key=lambda x: x[1]['final_score'], reverse=True):
                        marker = "ğŸ‘‘" if ai_name == best_ai_name else "  "
                        print(f"   {marker} {ai_name}: {eval_data['q_value']:.3f} Ã— {eval_data['priority']:.1f} = {eval_data['final_score']:.3f} | {eval_data['action']}")
                    print("-" * 60)
            
            time.sleep(0.4)
        
        print(f"âœ… ì•™ìƒë¸” í”Œë ˆì´ ì™„ë£Œ! ì´ {step} ì•¡ì…˜")
        print("\nğŸ† AI ì„ íƒ í†µê³„:")
        total_selections = sum(ai_selection_count.values())
        if total_selections > 0:
            for ai_name, count in sorted(ai_selection_count.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_selections) * 100
                print(f"   {ai_name}: {count}íšŒ ({percentage:.1f}%)")
    
    def ai_battle(self, duration=300):
        """AIë“¤ì´ ë²ˆê°ˆì•„ê°€ë©° í”Œë ˆì´í•˜ëŠ” ë°°í‹€"""
        print("âš”ï¸  AI ë°°í‹€ ëª¨ë“œ!")
        print(f"ğŸ• ì´ ì‹œê°„: {duration}ì´ˆ")
        print("=" * 40)
        
        ai_list = list(self.ais.items())
        current_ai_idx = 0
        switch_interval = 60  # 60ì´ˆë§ˆë‹¤ AI êµì²´
        
        start_time = time.time()
        last_switch = start_time
        
        while time.time() - start_time < duration:
            # AI êµì²´ ì‹œê°„ ì²´í¬
            if time.time() - last_switch >= switch_interval:
                current_ai_idx = (current_ai_idx + 1) % len(ai_list)
                last_switch = time.time()
                print(f"\nğŸ”„ AI êµì²´: {ai_list[current_ai_idx][0].upper()}")
            
            ai_name, ai = ai_list[current_ai_idx]
            
            state = self.client.get_game_state()
            if not state:
                time.sleep(0.5)
                continue
            
            # ê²Œì„ ì˜¤ë²„ ì²˜ë¦¬ (ê°•í™”ëœ ê°ì§€)
            is_game_over = state.get('isGameOver', False)
            
            # ì¶”ê°€ì ì¸ ê²Œì„ ì˜¤ë²„ ê°ì§€ ì¡°ê±´ë“¤
            if not is_game_over:
                board = state.get('board', [])
                if board:
                    # ìƒë‹¨ 3ì¤„ì— ë¸”ë¡ì´ ìˆëŠ”ì§€ í™•ì¸
                    for row in range(3):
                        if any(board[row][col] is not None and board[row][col] != 0 for col in range(10)):
                            is_game_over = True
                            print(f"ğŸš¨ {ai_name} AIê°€ ê²Œì„ ì˜¤ë²„ ê°ì§€: ìƒë‹¨ ì˜ì—­ ë¸”ë¡ ë°œê²¬ (í–‰ {row})")
                            break
            
            if is_game_over:
                print(f"ğŸ’€ {ai_name} ê²Œì„ ì˜¤ë²„ ê°ì§€! ì ìˆ˜: {state.get('score', 0)}, ë¼ì¸: {state.get('lines', 0)}")
                
                print(f"ğŸ”„ {ai_name} ì¦‰ì‹œ ì¬ì‹œì‘ ì‹œë„...")
                if self.client.send_action('restart'):
                    print(f"âœ… ì¬ì‹œì‘ ìš”ì²­ ì „ì†¡ë¨")
                    time.sleep(1.5)  # ì¬ì‹œì‘ ì²˜ë¦¬ ëŒ€ê¸°
                    
                    # ì¬ì‹œì‘ ì„±ê³µ í™•ì¸ (ì—¬ëŸ¬ ë²ˆ ì‹œë„)
                    for attempt in range(3):
                        time.sleep(0.5)
                        new_state = self.client.get_game_state()
                        if new_state and not new_state.get('isGameOver', False) and new_state.get('score', 0) == 0:
                            print(f"ğŸ® {ai_name} ìƒˆ ê²Œì„ ì‹œì‘ í™•ì¸ (ì‹œë„ {attempt + 1})")
                            break
                    else:
                        print(f"âš ï¸ {ai_name} ì¬ì‹œì‘ í™•ì¸ ì‹¤íŒ¨")
                else:
                    print(f"âŒ {ai_name} ì¬ì‹œì‘ ìš”ì²­ ì‹¤íŒ¨")
                continue
            
            # AI ì•¡ì…˜
            old_epsilon = ai.epsilon
            ai.epsilon = 0
            action_idx = ai.get_action(state)
            action = ai.actions[action_idx]
            ai.epsilon = old_epsilon
            
            self.client.send_action(action)
            time.sleep(0.4)
        
        print(f"ğŸ AI ë°°í‹€ ì™„ë£Œ!")

def main():
    if not PYTORCH_AVAILABLE:
        print("âŒ PyTorch ì„¤ì¹˜ í•„ìš”: pip install torch")
        return
    
    print("ğŸ§  PyTorch DQN ë‹¤ì¤‘ ì „ë¬¸í™” í…ŒíŠ¸ë¦¬ìŠ¤ AI")
    print("=" * 50)
    
    system = MultiAITetrisSystem()
    
    # ì„œë²„ ì—°ê²° í™•ì¸
    if not system.client.get_game_state():
        print("âŒ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ 'pnpm dev'ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•˜ê³  ë¸Œë¼ìš°ì €ì—ì„œ 'AI ëª¨ë“œ ON'ì„ í´ë¦­í•˜ì„¸ìš”.")
        return
    
    print("\nëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ì „ì²´ AI í›ˆë ¨")
    print("2. íŠ¹ì • AI í”Œë ˆì´")
    print("3. AI ë°°í‹€ ëª¨ë“œ")
    print("4. ìŠ¤ë§ˆíŠ¸ ì•™ìƒë¸” AI í”Œë ˆì´ (ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ìƒì¡´ ì „ëµ)")
    
    mode = input("ì„ íƒ (1-4): ")
    
    if mode == "1":
        episodes = int(input("ê° AI í›ˆë ¨ ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸ 30): ") or "30")
        system.train_all(episodes=episodes)
    
    elif mode == "2":
        print("\nì‚¬ìš© ê°€ëŠ¥í•œ AI:")
        for i, name in enumerate(system.ais.keys(), 1):
            print(f"{i}. {name}")
        
        ai_choice = input("AI ì„ íƒ (ì´ë¦„ ë˜ëŠ” ë²ˆí˜¸): ")
        
        # ë²ˆí˜¸ë¡œ ì„ íƒí•œ ê²½ìš°
        if ai_choice.isdigit():
            ai_names = list(system.ais.keys())
            ai_idx = int(ai_choice) - 1
            if 0 <= ai_idx < len(ai_names):
                ai_choice = ai_names[ai_idx]
        
        duration = int(input("í”Œë ˆì´ ì‹œê°„(ì´ˆ) (ê¸°ë³¸ 120): ") or "120")
        system.play_with_ai(ai_choice, duration)
    
    elif mode == "3":
        duration = int(input("ë°°í‹€ ì‹œê°„(ì´ˆ) (ê¸°ë³¸ 300): ") or "300")
        system.ai_battle(duration)
    
    elif mode == "4":
        print("\nğŸ¯ ìŠ¤ë§ˆíŠ¸ ì•™ìƒë¸” AI ì „ëµ ì„¤ëª…:")
        print("  â€¢ ìœ„í—˜ë„ ë¶„ì„: ê²Œì„ ì˜¤ë²„ ìœ„í—˜ì„±ì— ë”°ë¥¸ AI ìš°ì„ ìˆœìœ„ ì¡°ì •")
        print("  â€¢ ê³µê°„ í™œìš©: ë„“ì€ ê³µê°„ì´ ìˆì„ ë•Œ ìš°ì„  í™œìš©, ìœ„ì¹˜ ì¡°ì • ì¥ë ¤") 
        print("  â€¢ í˜•íƒœ ë§ì¶¤: í˜„ì¬ ë¸”ë¡ì´ ë³´ë“œì— ì˜ ë§ëŠ”ì§€ ë¶„ì„í•˜ì—¬ íšŒì „/ì´ë™ ê²°ì •")
        print("  â€¢ ìƒì¡´ ìš°ì„ : ìœ„í—˜ ìƒí™©ì—ì„œ drop/rotate ì‹ ì¤‘ ì„ íƒ, ê· ë“±í•œ ë†’ì´ ìœ ì§€ ìµœìš°ì„ ")
        print("  â€¢ ìƒí™©ë³„ ì „ëµ: ì•ˆì „/ìœ„í—˜/ê·¹ìœ„í—˜ ìƒí™©ì— ë”°ë¥¸ AI ì—­í•  ìë™ ì¡°ì •")
        print("")
        duration = int(input("ì•™ìƒë¸” í”Œë ˆì´ ì‹œê°„(ì´ˆ) (ê¸°ë³¸ 120): ") or "120")
        system.play_with_ensemble(duration)
    
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 