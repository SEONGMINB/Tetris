#!/usr/bin/env python3
"""
ê°œì„ ëœ ë³´ìƒ ì‹œìŠ¤í…œì„ ê°€ì§„ ê°•í™”í•™ìŠµ í…ŒíŠ¸ë¦¬ìŠ¤ AI
"""
import requests
import time
import numpy as np
import random
from collections import deque
import json
import os

class ImprovedQLearningAgent:
    def __init__(self, state_size=200, action_size=5, learning_rate=0.15, discount_factor=0.95, epsilon=1.0):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        
        self.q_table = {}
        self.memory = deque(maxlen=5000)  # ë©”ëª¨ë¦¬ ì¦ê°€
        self.actions = ['left', 'right', 'down', 'rotate', 'drop']
        
        # ì´ì „ ìƒíƒœ ì¶”ì 
        self.prev_holes = 0
        self.prev_max_height = 0
        self.prev_bumpiness = 0
        
    def state_to_key(self, state):
        """ê²Œì„ ìƒíƒœë¥¼ ë” ì •êµí•œ íŠ¹ì§•ìœ¼ë¡œ ë³€í™˜"""
        if not state or 'board' not in state:
            return "empty"
        
        board = state['board']
        
        # 1. ê° ì—´ì˜ ë†’ì´
        heights = self.get_column_heights(board)
        
        # 2. êµ¬ë© ê°œìˆ˜
        holes = self.count_holes(board)
        
        # 3. ë†’ì´ ì°¨ì´ (bumpiness)
        bumpiness = self.calculate_bumpiness(heights)
        
        # 4. ì™„ì„± ê°€ëŠ¥í•œ ë¼ì¸
        almost_complete = self.count_almost_complete_lines(board)
        
        # 5. ìµœëŒ€ ë†’ì´
        max_height = max(heights) if heights else 0
        
        # íŠ¹ì§•ì„ ê°„ë‹¨í•˜ê²Œ ê·¸ë£¹í™” (ìƒíƒœ ê³µê°„ ì¶•ì†Œ)
        height_group = min(max_height // 3, 6)  # 0-6 ê·¸ë£¹
        holes_group = min(holes // 2, 5)  # 0-5 ê·¸ë£¹
        bump_group = min(bumpiness // 3, 4)  # 0-4 ê·¸ë£¹
        
        key = f"h:{height_group}|holes:{holes_group}|bump:{bump_group}|lines:{almost_complete}"
        return key
    
    def get_column_heights(self, board):
        """ê° ì—´ì˜ ë†’ì´ ê³„ì‚°"""
        heights = []
        for col in range(10):
            height = 0
            for row in range(20):
                if board[row][col] is not None and board[row][col] != 0:
                    height = 20 - row
                    break
            heights.append(height)
        return heights
    
    def count_holes(self, board):
        """êµ¬ë© ê°œìˆ˜ ê³„ì‚°"""
        holes = 0
        for col in range(10):
            block_found = False
            for row in range(20):
                cell = board[row][col]
                if cell is not None and cell != 0:
                    block_found = True
                elif block_found and (cell is None or cell == 0):
                    holes += 1
        return holes
    
    def calculate_bumpiness(self, heights):
        """ë†’ì´ ì°¨ì´ì˜ í•© ê³„ì‚°"""
        bumpiness = 0
        for i in range(len(heights) - 1):
            bumpiness += abs(heights[i] - heights[i + 1])
        return bumpiness
    
    def count_almost_complete_lines(self, board):
        """ê±°ì˜ ì™„ì„±ëœ ë¼ì¸ ìˆ˜ ê³„ì‚°"""
        almost_complete = 0
        for row in range(20):
            filled_cells = sum(1 for cell in board[row] if cell is not None and cell != 0)
            if filled_cells >= 8:  # 8ì¹¸ ì´ìƒ ì±„ì›Œì§„ ë¼ì¸
                almost_complete += 1
        return almost_complete
    
    def get_action(self, state):
        """ì•¡ì…˜ ì„ íƒ (Îµ-greedy)"""
        if np.random.random() <= self.epsilon:
            return random.choice(range(self.action_size))
        
        state_key = self.state_to_key(state)
        if state_key not in self.q_table:
            self.q_table[state_key] = [0.0] * self.action_size
        
        return np.argmax(self.q_table[state_key])
    
    def calculate_reward(self, prev_state, current_state, action):
        """ê°œì„ ëœ ë³´ìƒ ê³„ì‚°"""
        if not prev_state or not current_state:
            return 0
        
        reward = 0
        
        # 1. ìƒì¡´ ë³´ìƒ (ë§¤ ìŠ¤í…ë§ˆë‹¤)
        if not current_state.get('isGameOver', False):
            reward += 1.0  # ìƒì¡´ ìì²´ì— ë³´ìƒ
        
        # 2. ì ìˆ˜ ì¦ê°€ ë³´ìƒ (ì¦í­)
        score_diff = current_state.get('score', 0) - prev_state.get('score', 0)
        reward += score_diff * 0.1  # ê¸°ì¡´ 0.01ì—ì„œ 0.1ë¡œ ì¦ê°€
        
        # 3. ë¼ì¸ í´ë¦¬ì–´ ëŒ€í­ ë³´ìƒ
        lines_diff = current_state.get('lines', 0) - prev_state.get('lines', 0)
        if lines_diff > 0:
            reward += lines_diff * 25  # ê¸°ì¡´ 10ì—ì„œ 25ë¡œ ì¦ê°€
            if lines_diff >= 4:  # í…ŒíŠ¸ë¦¬ìŠ¤!
                reward += 50  # ë³´ë„ˆìŠ¤
        
        # 4. ë³´ë“œ ìƒíƒœ ê¸°ë°˜ ë³´ìƒ/íŒ¨ë„í‹°
        if current_state.get('board'):
            board = current_state['board']
            heights = self.get_column_heights(board)
            holes = self.count_holes(board)
            max_height = max(heights) if heights else 0
            bumpiness = self.calculate_bumpiness(heights)
            
            # êµ¬ë© ìƒì„± íŒ¨ë„í‹°
            hole_diff = holes - self.prev_holes
            if hole_diff > 0:
                reward -= hole_diff * 8  # ìƒˆë¡œìš´ êµ¬ë©ë§ˆë‹¤ -8ì 
            
            # ë†’ì´ ê´€ë¦¬
            height_diff = max_height - self.prev_max_height
            if height_diff > 0:
                reward -= height_diff * 2  # ë†’ì´ ì¦ê°€ íŒ¨ë„í‹°
            elif height_diff < 0:
                reward += abs(height_diff) * 1  # ë†’ì´ ê°ì†Œ ë³´ìƒ
            
            # ë†’ì´ê°€ ë„ˆë¬´ ë†’ìœ¼ë©´ ê°•í•œ íŒ¨ë„í‹°
            if max_height > 16:
                reward -= (max_height - 16) * 5
            
            # ê· ë“±í•œ ë†’ì´ ë³´ìƒ
            bumpiness_diff = bumpiness - self.prev_bumpiness
            if bumpiness_diff < 0:
                reward += 2  # ë” ê· ë“±í•´ì§€ë©´ ë³´ìƒ
            elif bumpiness_diff > 0:
                reward -= 1  # ë” ë¶ˆê· ë“±í•´ì§€ë©´ íŒ¨ë„í‹°
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            self.prev_holes = holes
            self.prev_max_height = max_height
            self.prev_bumpiness = bumpiness
        
        # 5. íŠ¹ì • ì•¡ì…˜ì— ëŒ€í•œ í”¼ë“œë°±
        if action == 'drop':
            reward += 0.5  # í•˜ë“œ ë“œë¡­ ì•½ê°„ ì„ í˜¸
        elif action == 'down':
            reward += 0.2  # ë¹ ë¥¸ ë‚™í•˜ ì•½ê°„ ì„ í˜¸
        
        # 6. ê²Œì„ ì˜¤ë²„ íŒ¨ë„í‹° (ê°ì†Œ)
        if current_state.get('isGameOver', False):
            reward -= 30  # ê¸°ì¡´ 50ì—ì„œ 30ìœ¼ë¡œ ê°ì†Œ
        
        return reward
    
    def remember(self, state, action, reward, next_state, done):
        """ê²½í—˜ì„ ë©”ëª¨ë¦¬ì— ì €ì¥"""
        self.memory.append((state, action, reward, next_state, done))
    
    def replay(self, batch_size=64):  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
        """ê²½í—˜ ì¬ìƒì„ í†µí•œ í•™ìŠµ"""
        if len(self.memory) < batch_size:
            return
        
        batch = random.sample(self.memory, batch_size)
        
        for state, action, reward, next_state, done in batch:
            state_key = self.state_to_key(state)
            next_state_key = self.state_to_key(next_state)
            
            if state_key not in self.q_table:
                self.q_table[state_key] = [0.0] * self.action_size
            if next_state_key not in self.q_table:
                self.q_table[next_state_key] = [0.0] * self.action_size
            
            target = reward
            if not done:
                target += self.discount_factor * max(self.q_table[next_state_key])
            
            self.q_table[state_key][action] += self.learning_rate * (target - self.q_table[state_key][action])
        
        # epsilon ê°ì†Œ
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def save_model(self, filepath):
        """ëª¨ë¸ ì €ì¥"""
        model_data = {
            'q_table': self.q_table,
            'epsilon': self.epsilon
        }
        with open(filepath, 'w') as f:
            json.dump(model_data, f, indent=2)
    
    def load_model(self, filepath):
        """ëª¨ë¸ ë¡œë“œ"""
        if os.path.exists(filepath):
            with open(filepath, 'r') as f:
                model_data = json.load(f)
                self.q_table = model_data.get('q_table', {})
                self.epsilon = model_data.get('epsilon', self.epsilon)
            print(f"âœ… ëª¨ë¸ ë¡œë“œë¨: {len(self.q_table)}ê°œ ìƒíƒœ")
        else:
            print("âŒ ì €ì¥ëœ ëª¨ë¸ ì—†ìŒ, ìƒˆë¡œ í•™ìŠµ ì‹œì‘")

class TetrisAPIClient:
    def __init__(self, base_url="http://localhost:3000"):
        self.base_url = base_url
        self.api_url = f"{base_url}/api/game"
    
    def get_game_state(self):
        try:
            response = requests.get(self.api_url, timeout=5)
            if response.status_code == 200:
                data = response.json()
                return data.get('data', {})
        except:
            pass
        return None
    
    def send_action(self, action):
        try:
            response = requests.post(self.api_url, json={
                'type': 'action', 
                'action': action
            }, timeout=5)
            return response.status_code == 200
        except:
            return False

class ImprovedTetrisAI:
    def __init__(self):
        self.agent = ImprovedQLearningAgent()
        self.client = TetrisAPIClient()
        self.model_path = "improved_tetris_q_model.json"
        
        # ëª¨ë¸ ë¡œë“œ
        self.agent.load_model(self.model_path)
    
    def train(self, episodes=50, max_steps=500):  # ë” ì§§ì€ ì—í”¼ì†Œë“œ
        """ê°•í™”í•™ìŠµ í›ˆë ¨"""
        print("ğŸ§  ê°œì„ ëœ ê°•í™”í•™ìŠµ í›ˆë ¨ ì‹œì‘!")
        print(f"ğŸ“Š ì—í”¼ì†Œë“œ: {episodes}, ìµœëŒ€ ìŠ¤í…: {max_steps}")
        print("ğŸ ìƒˆë¡œìš´ ë³´ìƒ ì‹œìŠ¤í…œ:")
        print("   â€¢ ìƒì¡´ ë³´ìƒ: +1.0/ìŠ¤í…")
        print("   â€¢ ë¼ì¸ í´ë¦¬ì–´: +25/ë¼ì¸ (í…ŒíŠ¸ë¦¬ìŠ¤ +50 ë³´ë„ˆìŠ¤)")
        print("   â€¢ êµ¬ë© ìƒì„±: -8/êµ¬ë©")
        print("   â€¢ ë†’ì´ ê´€ë¦¬: Â±1-5ì ")
        print("=" * 60)
        
        for episode in range(episodes):
            print(f"\nğŸ® ì—í”¼ì†Œë“œ {episode + 1}/{episodes}")
            
            # ê²Œì„ ìƒíƒœ ì´ˆê¸°í™”
            prev_state = self.client.get_game_state()
            if not prev_state:
                print("âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨")
                continue
            
            # ê²Œì„ì´ ì´ë¯¸ ì˜¤ë²„ë©´ ë„˜ì–´ê°€ê¸°
            if prev_state.get('isGameOver', False):
                print("   âš ï¸ ê²Œì„ì´ ì´ë¯¸ ì˜¤ë²„ ìƒíƒœì…ë‹ˆë‹¤. ë¸Œë¼ìš°ì €ì—ì„œ ì¬ì‹œì‘í•˜ì„¸ìš”.")
                continue
            
            # ìƒíƒœ ì´ˆê¸°í™”
            if prev_state.get('board'):
                board = prev_state['board']
                heights = self.agent.get_column_heights(board)
                self.agent.prev_holes = self.agent.count_holes(board)
                self.agent.prev_max_height = max(heights) if heights else 0
                self.agent.prev_bumpiness = self.agent.calculate_bumpiness(heights)
            
            total_reward = 0
            step = 0
            
            for step in range(max_steps):
                # í˜„ì¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
                current_state = self.client.get_game_state()
                if not current_state:
                    break
                
                # ê²Œì„ ì˜¤ë²„ ì²´í¬
                if current_state.get('isGameOver', False):
                    print(f"   ğŸ’€ ê²Œì„ ì˜¤ë²„! ìŠ¤í…: {step} - ìë™ ì¬ì‹œì‘...")
                    # ìë™ìœ¼ë¡œ ì¬ì‹œì‘ ì•¡ì…˜ ì „ì†¡
                    if self.client.send_action('restart'):
                        print(f"   ğŸ”„ ê²Œì„ ì¬ì‹œì‘ë¨")
                        time.sleep(1)  # ì¬ì‹œì‘ í›„ ì ì‹œ ëŒ€ê¸°
                        continue  # ì—í”¼ì†Œë“œ ê³„ì† ì§„í–‰
                    else:
                        print(f"   âŒ ì¬ì‹œì‘ ì‹¤íŒ¨")
                        break
                
                # ì•¡ì…˜ ì„ íƒ
                action_idx = self.agent.get_action(current_state)
                action = self.agent.actions[action_idx]
                
                # ì•¡ì…˜ ì‹¤í–‰
                if not self.client.send_action(action):
                    continue
                
                time.sleep(0.15)  # ì•½ê°„ ë” ê¸´ ê°„ê²©
                
                # ë‹¤ìŒ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
                next_state = self.client.get_game_state()
                if not next_state:
                    continue
                
                # ë³´ìƒ ê³„ì‚°
                reward = self.agent.calculate_reward(current_state, next_state, action)
                total_reward += reward
                
                # ê²½í—˜ ì €ì¥
                done = next_state.get('isGameOver', False)
                self.agent.remember(current_state, action_idx, reward, next_state, done)
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if step % 30 == 0 and step > 0:  # ë” ìì£¼ ì¶œë ¥
                    score = next_state.get('score', 0)
                    lines = next_state.get('lines', 0)
                    print(f"   ğŸ“ˆ ìŠ¤í… {step}: ì ìˆ˜ {score}, ë¼ì¸ {lines}, ë³´ìƒ {reward:.1f} (ëˆ„ì : {total_reward:.1f})")
                
                prev_state = current_state
                
                if done:
                    break
            
            # í•™ìŠµ ìˆ˜í–‰
            self.agent.replay()
            
            # ì—í”¼ì†Œë“œ ê²°ê³¼ ì¶œë ¥
            final_score = current_state.get('score', 0) if current_state else 0
            final_lines = current_state.get('lines', 0) if current_state else 0
            
            print(f"   ğŸ¯ ì—í”¼ì†Œë“œ ì™„ë£Œ!")
            print(f"   ğŸ“Š ìµœì¢… ì ìˆ˜: {final_score}, ë¼ì¸: {final_lines}")
            print(f"   ğŸ ì´ ë³´ìƒ: {total_reward:.1f}")
            print(f"   ğŸ” íƒí—˜ë¥ : {self.agent.epsilon:.3f}")
            print(f"   ğŸ§  í•™ìŠµëœ ìƒíƒœ: {len(self.agent.q_table)}")
            
            # ì£¼ê¸°ì ìœ¼ë¡œ ëª¨ë¸ ì €ì¥
            if (episode + 1) % 5 == 0:  # ë” ìì£¼ ì €ì¥
                self.agent.save_model(self.model_path)
                print(f"   ğŸ’¾ ëª¨ë¸ ì €ì¥ë¨ (ì—í”¼ì†Œë“œ {episode + 1})")
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        self.agent.save_model(self.model_path)
        print(f"\nâœ… í›ˆë ¨ ì™„ë£Œ! ëª¨ë¸ ì €ì¥ë¨: {self.model_path}")
    
    def play(self, duration=120):
        """í•™ìŠµëœ AIë¡œ í”Œë ˆì´"""
        print("ğŸ¤– ê°œì„ ëœ AIê°€ í”Œë ˆì´í•©ë‹ˆë‹¤!")
        print(f"ğŸ• í”Œë ˆì´ ì‹œê°„: {duration}ì´ˆ")
        print(f"ğŸ” íƒí—˜ë¥ : {self.agent.epsilon:.3f}")
        print("=" * 40)
        
        start_time = time.time()
        step = 0
        
        while time.time() - start_time < duration:
            state = self.client.get_game_state()
            if not state:
                time.sleep(0.5)
                continue
            
            if state.get('isGameOver', False):
                print(f"ğŸ¯ ê²Œì„ ì˜¤ë²„! ì ìˆ˜: {state.get('score', 0)} - ìë™ ì¬ì‹œì‘...")
                # ìë™ìœ¼ë¡œ ì¬ì‹œì‘ ì•¡ì…˜ ì „ì†¡
                if self.client.send_action('restart'):
                    print(f"ğŸ”„ ê²Œì„ ì¬ì‹œì‘ë¨, ê³„ì† í”Œë ˆì´...")
                    time.sleep(1)  # ì¬ì‹œì‘ í›„ ì ì‹œ ëŒ€ê¸°
                    continue  # í”Œë ˆì´ ê³„ì† ì§„í–‰
                else:
                    print(f"âŒ ì¬ì‹œì‘ ì‹¤íŒ¨")
                    break
            
            # ìµœì  ì•¡ì…˜ ì„ íƒ (íƒí—˜ ì—†ì´)
            old_epsilon = self.agent.epsilon
            self.agent.epsilon = 0
            
            action_idx = self.agent.get_action(state)
            action = self.agent.actions[action_idx]
            
            self.agent.epsilon = old_epsilon
            
            # ì•¡ì…˜ ì‹¤í–‰
            if self.client.send_action(action):
                step += 1
                if step % 15 == 0:
                    score = state.get('score', 0)
                    lines = state.get('lines', 0)
                    print(f"ğŸ® ìŠ¤í… {step}: ì ìˆ˜ {score}, ë¼ì¸ {lines}, ì•¡ì…˜ {action}")
            
            time.sleep(0.2)
        
        print(f"âœ… í”Œë ˆì´ ì™„ë£Œ! ì´ {step} ì•¡ì…˜")

def main():
    print("ğŸ§  ê°œì„ ëœ ê°•í™”í•™ìŠµ í…ŒíŠ¸ë¦¬ìŠ¤ AI")
    print("=" * 40)
    
    ai = ImprovedTetrisAI()
    
    # ì„œë²„ ì—°ê²° í™•ì¸
    if not ai.client.get_game_state():
        print("âŒ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ 'pnpm dev'ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•˜ê³  ë¸Œë¼ìš°ì €ì—ì„œ 'AI ëª¨ë“œ ON'ì„ í´ë¦­í•˜ì„¸ìš”.")
        return
    
    mode = input("\nëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:\n1. í›ˆë ¨ (train)\n2. í”Œë ˆì´ (play)\nì„ íƒ (1 ë˜ëŠ” 2): ")
    
    if mode == "1":
        episodes = int(input("í›ˆë ¨ ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸ 30): ") or "30")
        ai.train(episodes=episodes)
    elif mode == "2":
        duration = int(input("í”Œë ˆì´ ì‹œê°„(ì´ˆ) (ê¸°ë³¸ 120): ") or "120")
        ai.play(duration=duration)
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 