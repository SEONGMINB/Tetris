#!/usr/bin/env python3
"""
ë‹¨ìˆœí•œ ë³´ìƒ ì‹œìŠ¤í…œ ê°•í™”í•™ìŠµ í…ŒíŠ¸ë¦¬ìŠ¤ AI
- ë†’ì´ ê°ì†Œ: +1ì /ì¹¸
- ë¼ì¸ í´ë¦¬ì–´: +20ì /ë¼ì¸
"""
import requests
import time
import numpy as np
import random
from collections import deque
import json
import os

class SimpleRewardAgent:
    def __init__(self, learning_rate=0.2, discount_factor=0.9, epsilon=1.0):
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.epsilon_min = 0.05
        self.epsilon_decay = 0.998
        
        self.q_table = {}
        self.memory = deque(maxlen=3000)
        self.actions = ['left', 'right', 'down', 'rotate', 'drop']
        
    def get_max_height(self, board):
        """ë³´ë“œì˜ ìµœëŒ€ ë†’ì´ ê³„ì‚°"""
        max_height = 0
        for col in range(10):
            for row in range(20):
                if board[row][col] is not None and board[row][col] != 0:
                    height = 20 - row
                    max_height = max(max_height, height)
                    break
        return max_height
    
    def get_piece_placement_rows(self, prev_board, current_board):
        """ìƒˆë¡œ ë°°ì¹˜ëœ í”¼ìŠ¤ê°€ ë†“ì¸ í–‰ë“¤ì„ ì°¾ê¸°"""
        if not prev_board or not current_board:
            return []
        
        placed_rows = []
        for row in range(20):
            for col in range(10):
                prev_cell = prev_board[row][col]
                current_cell = current_board[row][col]
                
                # ì´ì „ì— ì—†ë˜ ë¸”ëŸ­ì´ ìƒˆë¡œ ìƒê²¼ìœ¼ë©´
                if (prev_cell is None or prev_cell == 0) and (current_cell is not None and current_cell != 0):
                    if row not in placed_rows:
                        placed_rows.append(row)
        
        return placed_rows
    
    def count_holes(self, board):
        """ë³´ë“œì—ì„œ êµ¬ë©(ìœ„ì— ë¸”ë¡ì´ ìˆëŠ”ë° ì•„ë˜ ë¹ˆ ê³µê°„) ê°œìˆ˜ ê³„ì‚°"""
        if not board:
            return 0
        
        holes = 0
        for col in range(10):
            block_found = False
            for row in range(20):
                if board[row][col] is not None and board[row][col] != 0:
                    block_found = True
                elif block_found and (board[row][col] is None or board[row][col] == 0):
                    holes += 1
        
        return holes
    
    def calculate_hole_filling_reward(self, prev_state, current_state):
        """êµ¬ë© ë©”ìš°ê¸° ë³´ìƒ ê³„ì‚°"""
        if not prev_state.get('board') or not current_state.get('board'):
            return 0
        
        prev_holes = self.count_holes(prev_state['board'])
        current_holes = self.count_holes(current_state['board'])
        
        holes_filled = prev_holes - current_holes
        
        if holes_filled > 0:
            # êµ¬ë©ì„ ë©”ìš¸ ë•Œë§ˆë‹¤ 50ì ì”© ì¶”ê°€
            hole_reward = holes_filled * 50
            print(f"   ğŸ§© êµ¬ë© ë©”ìš°ê¸°! {holes_filled}ê°œ êµ¬ë© = +{hole_reward}ì ")
            return hole_reward
        
        return 0
    
    def get_column_heights(self, board):
        """ê° ì—´ì˜ ë†’ì´ë¥¼ ê³„ì‚°"""
        if not board:
            return [0] * 10
        
        heights = []
        for col in range(10):
            height = 0
            for row in range(20):
                if board[row][col] is not None and board[row][col] != 0:
                    height = 20 - row
                    break
            heights.append(height)
        return heights
    
    def calculate_tetris_setup_reward(self, current_state):
        """í…ŒíŠ¸ë¦¬ìŠ¤ ì…‹ì—… ë³´ìƒ - I-pieceë¡œ í´ë¦¬ì–´í•  ìˆ˜ ìˆëŠ” ëª¨ì–‘ ë§Œë“¤ê¸°"""
        if not current_state.get('board'):
            return 0
        
        board = current_state['board']
        heights = self.get_column_heights(board)
        
        # í…ŒíŠ¸ë¦¬ìŠ¤ ì…‹ì—… ê°ì§€: í•œ ì—´ë§Œ ë‚®ê³  ë‚˜ë¨¸ì§€ ì—´ë“¤ì´ ê±°ì˜ ê°™ì€ ë†’ì´
        setup_reward = 0
        
        for target_col in range(10):
            target_height = heights[target_col]
            other_heights = [h for i, h in enumerate(heights) if i != target_col]
            
            if not other_heights:
                continue
            
            # ë‹¤ë¥¸ ì—´ë“¤ì˜ í‰ê·  ë†’ì´
            avg_other_height = sum(other_heights) / len(other_heights)
            
            # ì¡°ê±´: íƒ€ê²Ÿ ì—´ì´ ë‹¤ë¥¸ ì—´ë“¤ë³´ë‹¤ 3-4ì¹¸ ë‚®ê³ , ë‹¤ë¥¸ ì—´ë“¤ì´ ë¹„ìŠ·í•œ ë†’ì´
            height_diff = avg_other_height - target_height
            
            if 3 <= height_diff <= 4:
                # ë‹¤ë¥¸ ì—´ë“¤ì´ ì–¼ë§ˆë‚˜ ê³ ë¥´ê²Œ ìŒ“ì˜€ëŠ”ì§€ í™•ì¸
                height_variance = sum((h - avg_other_height) ** 2 for h in other_heights) / len(other_heights)
                
                if height_variance <= 1.0:  # ë†’ì´ ì°¨ì´ê°€ ì ì„ ë•Œ
                    setup_points = int(height_diff * 30)  # 3ì¹¸ ì°¨ì´ = 90ì , 4ì¹¸ ì°¨ì´ = 120ì 
                    setup_reward += setup_points
                    print(f"   ğŸ¯ í…ŒíŠ¸ë¦¬ìŠ¤ ì…‹ì—…! {target_col+1}ë²ˆ ì—´ ëŒ€ê¸° = +{setup_points}ì ")
        
        return setup_reward
    
    def calculate_flatness_reward(self, prev_state, current_state):
        """í‰íƒ„ì„± ë³´ìƒ - ë„¤ëª¨ì§„ ëª¨ì–‘ìœ¼ë¡œ ìŒ“ì´ë©´ ë³´ìƒ"""
        if not prev_state.get('board') or not current_state.get('board'):
            return 0
        
        prev_heights = self.get_column_heights(prev_state['board'])
        current_heights = self.get_column_heights(current_state['board'])
        
        # ì´ì „ê³¼ í˜„ì¬ì˜ ë†’ì´ ë¶„ì‚° ê³„ì‚°
        def calculate_height_variance(heights):
            if not heights:
                return 0
            avg_height = sum(heights) / len(heights)
            return sum((h - avg_height) ** 2 for h in heights) / len(heights)
        
        prev_variance = calculate_height_variance(prev_heights)
        current_variance = calculate_height_variance(current_heights)
        
        # ë¶„ì‚°ì´ ì¤„ì–´ë“¤ì—ˆìœ¼ë©´ (ë” í‰íƒ„í•´ì¡Œìœ¼ë©´) ë³´ìƒ
        variance_improvement = prev_variance - current_variance
        
        if variance_improvement > 0:
            flatness_reward = int(variance_improvement * 20)  # ë¶„ì‚° ê°œì„  ì •ë„ì— ë¹„ë¡€
            print(f"   ğŸ“ í‰íƒ„í™”! ë†’ì´ ê· ë“±í™” = +{flatness_reward}ì ")
            return flatness_reward
        
        return 0

    def calculate_depth_reward(self, prev_state, current_state):
        """ê¹Šì´ ë°°ì¹˜ ë³´ìƒ ê³„ì‚° - ì•„ë˜ìª½ì— ë†“ì„ìˆ˜ë¡ ë” ë†’ì€ ì ìˆ˜"""
        if not prev_state.get('board') or not current_state.get('board'):
            return 0
        
        prev_board = prev_state['board']
        current_board = current_state['board']
        
        # ìƒˆë¡œ ë°°ì¹˜ëœ í”¼ìŠ¤ì˜ í–‰ë“¤ ì°¾ê¸°
        placed_rows = self.get_piece_placement_rows(prev_board, current_board)
        
        if not placed_rows:
            return 0
        
        total_depth_reward = 0
        
        for row in placed_rows:
            # ê¹Šì´ ë³´ìƒ: ë§¨ ì•„ë˜(row 19) = 20ì , ìœ„ë¡œ ê°ˆìˆ˜ë¡ ê°ì†Œ
            depth_score = 20 - row  # row 19 = 1ì , row 18 = 2ì , ..., row 0 = 20ì 
            
            # ì¶”ê°€ ë³´ë„ˆìŠ¤: ì •ë§ ê¹Šì€ ê³³ (15í–‰ ì´í•˜)ì— ë†“ìœ¼ë©´ ë³´ë„ˆìŠ¤
            if row >= 15:  # ì•„ë˜ìª½ 5ì¤„
                depth_bonus = (row - 14) * 2  # 15í–‰=2ì , 16í–‰=4ì , ..., 19í–‰=10ì 
                depth_score += depth_bonus
                total_depth_reward += depth_score
                print(f"   ğŸ”ï¸ ê¹Šì´ ë°°ì¹˜! {20-row}ë²ˆì§¸ ì¤„ = +{depth_score}ì  (ê¹Šì´ë³´ë„ˆìŠ¤ +{depth_bonus})")
            else:
                total_depth_reward += depth_score
                print(f"   â¬‡ï¸ ë°°ì¹˜! {20-row}ë²ˆì§¸ ì¤„ = +{depth_score}ì ")
        
        return total_depth_reward
    
    def state_to_key(self, state):
        """ê²Œì„ ìƒíƒœë¥¼ ê°„ë‹¨í•œ í‚¤ë¡œ ë³€í™˜"""
        if not state or 'board' not in state:
            return "empty"
        
        board = state['board']
        max_height = self.get_max_height(board)
        
        # ê° ì—´ì˜ ë†’ì´ (ìµœëŒ€ 5ì¹¸ê¹Œì§€ë§Œ ê³ ë ¤)
        heights = []
        for col in range(10):
            height = 0
            for row in range(20):
                if board[row][col] is not None and board[row][col] != 0:
                    height = min(20 - row, 10)  # ìµœëŒ€ 10ìœ¼ë¡œ ì œí•œ
                    break
            heights.append(height)
        
        # ë†’ì´ë¥¼ ê·¸ë£¹í™” (ìƒíƒœ ê³µê°„ ì¶•ì†Œ)
        height_groups = [min(h // 2, 5) for h in heights]  # 0-5 ê·¸ë£¹
        max_height_group = min(max_height // 3, 6)  # 0-6 ê·¸ë£¹
        
        # í˜„ì¬ í”¼ìŠ¤ íƒ€ì…
        piece_type = state.get('currentPiece', {}).get('type', 'None')
        
        key = f"max_h:{max_height_group}|h:{'-'.join(map(str, height_groups[:5]))}|piece:{piece_type}"
        return key
    
    def get_action(self, state):
        """ì•¡ì…˜ ì„ íƒ (Îµ-greedy)"""
        if np.random.random() <= self.epsilon:
            return random.choice(range(len(self.actions)))
        
        state_key = self.state_to_key(state)
        if state_key not in self.q_table:
            self.q_table[state_key] = [0.0] * len(self.actions)
        
        return np.argmax(self.q_table[state_key])
    
    def calculate_reward(self, prev_state, current_state):
        """ê°„ë‹¨í•œ ë³´ìƒ ê³„ì‚°"""
        if not prev_state or not current_state:
            return 0
        
        reward = 0
        
        # 1. ê°œì„ ëœ ë¼ì¸ í´ë¦¬ì–´ ë³´ìƒ ì‹œìŠ¤í…œ
        prev_lines = prev_state.get('lines', 0)
        current_lines = current_state.get('lines', 0)
        lines_cleared = current_lines - prev_lines
        
        if lines_cleared > 0:
            # ìƒˆë¡œìš´ ë³´ìƒ ì‹œìŠ¤í…œ: ë” ë§ì€ ì¤„ì„ í•œë²ˆì— í´ë¦¬ì–´í• ìˆ˜ë¡ ë” í° ë³´ìƒ
            if lines_cleared == 1:
                line_reward = 100
                emoji = "ğŸŸ¡"
                name = "ì‹±ê¸€"
            elif lines_cleared == 2:
                line_reward = 200
                emoji = "ğŸŸ "
                name = "ë”ë¸”"
            elif lines_cleared == 3:
                line_reward = 300
                emoji = "ğŸ”´"
                name = "íŠ¸ë¦¬í”Œ"
            elif lines_cleared == 4:
                line_reward = 500  # í…ŒíŠ¸ë¦¬ìŠ¤ëŠ” íŠ¹ë³„íˆ ë” í° ë³´ìƒ!
                emoji = "ğŸ’"
                name = "í…ŒíŠ¸ë¦¬ìŠ¤"
            else:
                line_reward = lines_cleared * 100  # í˜¹ì‹œ 5ì¤„ ì´ìƒì´ë©´
                emoji = "ğŸŒŸ"
                name = "ìŠˆí¼"
            
            reward += line_reward
            print(f"   {emoji} {name}! {lines_cleared}ì¤„ í´ë¦¬ì–´ = +{line_reward}ì ")
        
        # 2. ê°•í™”ëœ ë†’ì´ ê´€ë¦¬ ë³´ìƒ
        if current_state.get('board'):
            current_max_height = self.get_max_height(current_state['board'])
            prev_max_height = self.get_max_height(prev_state['board']) if prev_state.get('board') else 0
            
            # ê¸°ì¡´ ë†’ì´ ê°ì†Œ ë³´ìƒ (ê°•í™”)
            height_decrease = prev_max_height - current_max_height
            if height_decrease > 0:
                height_reward = height_decrease * 3  # ê¸°ì¡´ 1ì ì—ì„œ 3ì ìœ¼ë¡œ ì¦ê°€
                reward += height_reward
                print(f"   ğŸ“‰ ë†’ì´ ê°ì†Œ! {height_decrease}ì¹¸ = +{height_reward}ì ")
            
            # ìƒˆë¡œìš´ ê¹Šì´ ë°°ì¹˜ ë³´ìƒ ì‹œìŠ¤í…œ
            depth_reward = self.calculate_depth_reward(prev_state, current_state)
            if depth_reward > 0:
                reward += depth_reward
            
            # êµ¬ë© ë©”ìš°ê¸° ë³´ìƒ ì‹œìŠ¤í…œ
            hole_filling_reward = self.calculate_hole_filling_reward(prev_state, current_state)
            if hole_filling_reward > 0:
                reward += hole_filling_reward
            
            # í…ŒíŠ¸ë¦¬ìŠ¤ ì…‹ì—… ë³´ìƒ ì‹œìŠ¤í…œ (I-pieceë¡œ í´ë¦¬ì–´ ê°€ëŠ¥í•œ í˜•íƒœ)
            tetris_setup_reward = self.calculate_tetris_setup_reward(current_state)
            if tetris_setup_reward > 0:
                reward += tetris_setup_reward
            
            # í‰íƒ„ì„± ë³´ìƒ ì‹œìŠ¤í…œ (ë„¤ëª¨ì§„ ëª¨ì–‘ìœ¼ë¡œ ê· ë“±í•˜ê²Œ ìŒ“ê¸°)
            flatness_reward = self.calculate_flatness_reward(prev_state, current_state)
            if flatness_reward > 0:
                reward += flatness_reward
        
        # 3. ê²Œì„ ì˜¤ë²„ ì‹œ ì‘ì€ íŒ¨ë„í‹°
        if current_state.get('isGameOver', False):
            reward -= 5
            print(f"   ğŸ’€ ê²Œì„ ì˜¤ë²„ = -5ì ")
        
        return reward
    
    def remember(self, state, action, reward, next_state, done):
        """ê²½í—˜ì„ ë©”ëª¨ë¦¬ì— ì €ì¥"""
        self.memory.append((state, action, reward, next_state, done))
    
    def replay(self, batch_size=32):
        """ê²½í—˜ ì¬ìƒì„ í†µí•œ í•™ìŠµ"""
        if len(self.memory) < batch_size:
            return
        
        batch = random.sample(self.memory, batch_size)
        
        for state, action, reward, next_state, done in batch:
            state_key = self.state_to_key(state)
            next_state_key = self.state_to_key(next_state)
            
            if state_key not in self.q_table:
                self.q_table[state_key] = [0.0] * len(self.actions)
            if next_state_key not in self.q_table:
                self.q_table[next_state_key] = [0.0] * len(self.actions)
            
            target = reward
            if not done:
                target += self.discount_factor * max(self.q_table[next_state_key])
            
            # Q-ê°’ ì—…ë°ì´íŠ¸
            old_value = self.q_table[state_key][action]
            self.q_table[state_key][action] += self.learning_rate * (target - old_value)
        
        # epsilon ê°ì†Œ (íƒí—˜ -> í™œìš©)
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def save_model(self, filepath):
        """ëª¨ë¸ ì €ì¥"""
        model_data = {
            'q_table': self.q_table,
            'epsilon': self.epsilon
        }
        with open(filepath, 'w') as f:
            json.dump(model_data, f, indent=2)
    
    def load_model(self, filepath):
        """ëª¨ë¸ ë¡œë“œ"""
        if os.path.exists(filepath):
            with open(filepath, 'r') as f:
                model_data = json.load(f)
                self.q_table = model_data.get('q_table', {})
                self.epsilon = model_data.get('epsilon', self.epsilon)
            print(f"âœ… ëª¨ë¸ ë¡œë“œë¨: {len(self.q_table)}ê°œ ìƒíƒœ")
        else:
            print("âŒ ì €ì¥ëœ ëª¨ë¸ ì—†ìŒ, ìƒˆë¡œ í•™ìŠµ ì‹œì‘")

class TetrisAPIClient:
    def __init__(self, base_url="http://localhost:3000"):
        self.base_url = base_url
        self.api_url = f"{base_url}/api/game"
    
    def get_game_state(self):
        try:
            response = requests.get(self.api_url, timeout=5)
            if response.status_code == 200:
                data = response.json()
                return data.get('data', {})
        except:
            pass
        return None
    
    def send_action(self, action):
        try:
            response = requests.post(self.api_url, json={
                'type': 'action', 
                'action': action
            }, timeout=5)
            return response.status_code == 200
        except:
            return False

class SimpleRewardTetrisAI:
    def __init__(self):
        self.agent = SimpleRewardAgent()
        self.client = TetrisAPIClient()
        self.model_path = "simple_reward_tetris_model.json"
        
        # ëª¨ë¸ ë¡œë“œ
        self.agent.load_model(self.model_path)
    
    def wait_for_game_restart(self):
        """ìë™ìœ¼ë¡œ ê²Œì„ì„ ì¬ì‹œì‘í•˜ê³  ì‹œì‘ì„ ê¸°ë‹¤ë¦¬ëŠ” í•¨ìˆ˜"""
        print("   â³ ê²Œì„ ì˜¤ë²„! AIê°€ ìë™ìœ¼ë¡œ ì¬ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # ìë™ìœ¼ë¡œ restart ì•¡ì…˜ ë³´ë‚´ê¸°
        restart_success = self.client.send_action('restart')
        if restart_success:
            print("   ğŸ”„ ì¬ì‹œì‘ ëª…ë ¹ ì „ì†¡ë¨! ìƒˆ ê²Œì„ ì‹œì‘ ëŒ€ê¸° ì¤‘...", end="", flush=True)
        else:
            print("   âŒ ì¬ì‹œì‘ ëª…ë ¹ ì „ì†¡ ì‹¤íŒ¨! ìˆ˜ë™ ì¬ì‹œì‘ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤...")
            print("   ğŸ”„ ë¸Œë¼ìš°ì €ì—ì„œ R í‚¤ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”...", end="", flush=True)
        
        start_wait_time = time.time()
        dots = 0
        
        while True:
            time.sleep(1)  # 1ì´ˆë§ˆë‹¤ ì²´í¬ (ë” ë¹ ë¥´ê²Œ)
            state = self.client.get_game_state()
            
            if state and not state.get('isGameOver', False):
                # ê²Œì„ì´ ì¬ì‹œì‘ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ì ìˆ˜ì™€ ë¼ì¸ì´ ë¦¬ì…‹ë˜ì—ˆëŠ”ì§€)
                if state.get('score', 0) == 0 and state.get('lines', 0) == 0:
                    print("\n   âœ… ìƒˆ ê²Œì„ ì‹œì‘ë¨!")
                    return state
            
            # ëŒ€ê¸° ì¤‘ í‘œì‹œ
            dots = (dots + 1) % 4
            print("." * dots + " " * (3 - dots), end="\r", flush=True)
            
            # íƒ€ì„ì•„ì›ƒ (15ì´ˆë¡œ ë‹¨ì¶•)
            if time.time() - start_wait_time > 15:
                print("\n   â° 15ì´ˆ íƒ€ì„ì•„ì›ƒ. ë‹¤ìŒ ì—í”¼ì†Œë“œë¡œ ê±´ë„ˆëœë‹ˆë‹¤.")
                return None

    def train(self, episodes=500, max_steps=200):
        """ê°•í™”í•™ìŠµ í›ˆë ¨"""
        print("ğŸ§  ë‹¨ìˆœ ë³´ìƒ ì‹œìŠ¤í…œ ê°•í™”í•™ìŠµ í›ˆë ¨!")
        print("=" * 50)
        print("ğŸ ê¹Šì´ ì¤‘ì‹¬ ë³´ìƒ ì‹œìŠ¤í…œ:")
        print("   â€¢ ë†’ì´ ê°ì†Œ: +3ì /ì¹¸ (ê°•í™”)")
        print("   â€¢ ê¹Šì´ ë°°ì¹˜: 1ì¤„=20ì , 2ì¤„=19ì , ..., 20ì¤„=1ì ")
        print("   â€¢ ê¹Šì´ ë³´ë„ˆìŠ¤: ì•„ë˜ 5ì¤„ì— ë°°ì¹˜ì‹œ ì¶”ê°€ 2-10ì ")
        print("   â€¢ 1ì¤„ í´ë¦¬ì–´: +100ì  ğŸŸ¡")
        print("   â€¢ 2ì¤„ í´ë¦¬ì–´: +200ì  ğŸŸ ")
        print("   â€¢ 3ì¤„ í´ë¦¬ì–´: +300ì  ğŸ”´")
        print("   â€¢ 4ì¤„ í´ë¦¬ì–´: +500ì  ğŸ’ (í…ŒíŠ¸ë¦¬ìŠ¤!)")
        print("   â€¢ ê²Œì„ ì˜¤ë²„: -5ì ")
        print("ğŸ’¡ ê²Œì„ ì˜¤ë²„ ì‹œ ìë™ìœ¼ë¡œ ì¬ì‹œì‘ ëŒ€ê¸°í•©ë‹ˆë‹¤!")
        print("=" * 50)
        
        for episode in range(episodes):
            print(f"\nğŸ® ì—í”¼ì†Œë“œ {episode + 1}/{episodes}")
            
            # ê²Œì„ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
            prev_state = self.client.get_game_state()
            if not prev_state:
                print("âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨")
                continue
            
            # ê²Œì„ì´ ì´ë¯¸ ì˜¤ë²„ë©´ ì¬ì‹œì‘ ëŒ€ê¸°
            if prev_state.get('isGameOver', False):
                prev_state = self.wait_for_game_restart()
                if not prev_state:
                    continue
            
            total_reward = 0
            step = 0
            last_reward_step = 0
            
            print(f"   ğŸš€ ì‹œì‘! í˜„ì¬ ì ìˆ˜: {prev_state.get('score', 0)}, ë¼ì¸: {prev_state.get('lines', 0)}")
            
            for step in range(max_steps):
                # í˜„ì¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
                current_state = self.client.get_game_state()
                if not current_state:
                    print("   âŒ ìƒíƒœ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨")
                    break
                
                # ê²Œì„ ì˜¤ë²„ ì²´í¬
                if current_state.get('isGameOver', False):
                    print(f"   ğŸ’€ ê²Œì„ ì˜¤ë²„! ìŠ¤í…: {step}")
                    # ê²Œì„ ì˜¤ë²„ë„ í•™ìŠµì— í¬í•¨
                    reward = self.agent.calculate_reward(prev_state, current_state)
                    if reward != 0:
                        total_reward += reward
                    
                    # í•™ìŠµì— ê²Œì„ ì˜¤ë²„ ìƒíƒœ ì €ì¥
                    self.agent.remember(prev_state, action_idx, reward, current_state, True)
                    break
                
                # ì•¡ì…˜ ì„ íƒ
                action_idx = self.agent.get_action(current_state)
                action = self.agent.actions[action_idx]
                
                # ì•¡ì…˜ ì‹¤í–‰
                if not self.client.send_action(action):
                    continue
                
                time.sleep(0.1)  # ì ë‹¹í•œ ê°„ê²©
                
                # ë‹¤ìŒ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
                next_state = self.client.get_game_state()
                if not next_state:
                    continue
                
                # ë³´ìƒ ê³„ì‚°
                reward = self.agent.calculate_reward(current_state, next_state)
                if reward != 0:
                    total_reward += reward
                    last_reward_step = step
                    print(f"      â­ ìŠ¤í… {step}: ë³´ìƒ {reward:+.1f} (ëˆ„ì : {total_reward:.1f})")
                
                # ê²½í—˜ ì €ì¥
                done = next_state.get('isGameOver', False)
                self.agent.remember(current_state, action_idx, reward, next_state, done)
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if step % 40 == 0 and step > 0:
                    score = next_state.get('score', 0)
                    lines = next_state.get('lines', 0)
                    print(f"   ğŸ“Š ìŠ¤í… {step}: ì ìˆ˜ {score}, ë¼ì¸ {lines}")
                
                prev_state = current_state
                
                if done:
                    break
            
            # í•™ìŠµ ìˆ˜í–‰
            self.agent.replay()
            
            # ì—í”¼ì†Œë“œ ê²°ê³¼
            final_state = self.client.get_game_state()
            if final_state:
                final_score = final_state.get('score', 0)
                final_lines = final_state.get('lines', 0)
                print(f"   ğŸ¯ ì—í”¼ì†Œë“œ ì™„ë£Œ!")
                print(f"   ğŸ“Š ìµœì¢… ì ìˆ˜: {final_score}, ë¼ì¸: {final_lines}")
                print(f"   ğŸ ì´ ë³´ìƒ: {total_reward:.1f}")
                print(f"   ğŸ” íƒí—˜ë¥ : {self.agent.epsilon:.3f}")
                print(f"   ğŸ§  í•™ìŠµëœ ìƒíƒœ: {len(self.agent.q_table)}")
                print(f"   â° ë§ˆì§€ë§‰ ë³´ìƒ: ìŠ¤í… {last_reward_step}")
            
            # ì£¼ê¸°ì ìœ¼ë¡œ ëª¨ë¸ ì €ì¥
            if (episode + 1) % 5 == 0:
                self.agent.save_model(self.model_path)
                print(f"   ğŸ’¾ ëª¨ë¸ ì €ì¥ë¨ (ì—í”¼ì†Œë“œ {episode + 1})")
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        self.agent.save_model(self.model_path)
        print(f"\nâœ… í›ˆë ¨ ì™„ë£Œ! ëª¨ë¸ ì €ì¥ë¨: {self.model_path}")
    
    def play(self, duration=120):
        """í•™ìŠµëœ AIë¡œ í”Œë ˆì´"""
        print("ğŸ¤– í•™ìŠµëœ AIê°€ í”Œë ˆì´í•©ë‹ˆë‹¤!")
        print(f"ğŸ• í”Œë ˆì´ ì‹œê°„: {duration}ì´ˆ")
        print(f"ğŸ” íƒí—˜ë¥ : {self.agent.epsilon:.3f}")
        print("=" * 40)
        
        start_time = time.time()
        step = 0
        last_score = 0
        last_lines = 0
        
        while time.time() - start_time < duration:
            state = self.client.get_game_state()
            if not state:
                time.sleep(0.5)
                continue
            
            if state.get('isGameOver', False):
                print(f"ğŸ¯ ê²Œì„ ì˜¤ë²„! ì ìˆ˜: {state.get('score', 0)}")
                print("ğŸ”„ ê²Œì„ ì¬ì‹œì‘ì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...")
                
                # ì¬ì‹œì‘ ëŒ€ê¸°
                restart_state = self.wait_for_game_restart()
                if restart_state:
                    print("âœ… ê²Œì„ ì¬ì‹œì‘ë¨! í”Œë ˆì´ ê³„ì†...")
                    last_score = 0
                    last_lines = 0
                    continue
                else:
                    break
            
            # í•™ìŠµëœ ì •ì±…ìœ¼ë¡œ ì•¡ì…˜ ì„ íƒ (íƒí—˜ ì—†ì´)
            old_epsilon = self.agent.epsilon
            self.agent.epsilon = 0.05  # ì•½ê°„ì˜ ëœë¤ì„± ìœ ì§€
            
            action_idx = self.agent.get_action(state)
            action = self.agent.actions[action_idx]
            
            self.agent.epsilon = old_epsilon
            
            # ì•¡ì…˜ ì‹¤í–‰
            if self.client.send_action(action):
                step += 1
                
                current_score = state.get('score', 0)
                current_lines = state.get('lines', 0)
                
                # ì ìˆ˜ë‚˜ ë¼ì¸ì´ ë³€í–ˆì„ ë•Œ ì¶œë ¥
                if current_score != last_score or current_lines != last_lines:
                    score_diff = current_score - last_score
                    lines_diff = current_lines - last_lines
                    print(f"ğŸ® ìŠ¤í… {step}: ì ìˆ˜ {current_score} (+{score_diff}), ë¼ì¸ {current_lines} (+{lines_diff}), ì•¡ì…˜ {action}")
                    last_score = current_score
                    last_lines = current_lines
                elif step % 50 == 0:  # 50ìŠ¤í…ë§ˆë‹¤ ìƒíƒœ ì¶œë ¥
                    print(f"ğŸ® ìŠ¤í… {step}: ì ìˆ˜ {current_score}, ë¼ì¸ {current_lines}, ì•¡ì…˜ {action}")
            
            time.sleep(0.15)
        
        print(f"âœ… í”Œë ˆì´ ì™„ë£Œ! ì´ {step} ì•¡ì…˜")

def main():
    print("ğŸ§  ë‹¨ìˆœ ë³´ìƒ ì‹œìŠ¤í…œ ê°•í™”í•™ìŠµ í…ŒíŠ¸ë¦¬ìŠ¤ AI")
    print("=" * 50)
    
    ai = SimpleRewardTetrisAI()
    
    # ì„œë²„ ì—°ê²° í™•ì¸
    state = ai.client.get_game_state()
    if not state:
        print("âŒ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ í•´ê²°ì±…:")
        print("   1. í„°ë¯¸ë„ì—ì„œ 'pnpm dev' ì‹¤í–‰")
        print("   2. ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:3000 ì ‘ì†")
        print("   3. 'ğŸ¤– AI ëª¨ë“œ ON' ë²„íŠ¼ í´ë¦­")
        return
    
    # í˜„ì¬ ê²Œì„ ìƒíƒœ í‘œì‹œ
    print(f"ğŸ® í˜„ì¬ ê²Œì„ ìƒíƒœ:")
    print(f"   ì ìˆ˜: {state.get('score', 0)}")
    print(f"   ë¼ì¸: {state.get('lines', 0)}")
    print(f"   ê²Œì„ì˜¤ë²„: {state.get('isGameOver', False)}")
    
    mode = input("\nëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:\n1. í›ˆë ¨ (train)\n2. í”Œë ˆì´ (play)\nì„ íƒ (1 ë˜ëŠ” 2): ")
    
    if mode == "1":
        episodes = int(input("í›ˆë ¨ ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸ 500): ") or "500")
        ai.train(episodes=episodes)
    elif mode == "2":
        duration = int(input("í”Œë ˆì´ ì‹œê°„(ì´ˆ) (ê¸°ë³¸ 120): ") or "120")
        ai.play(duration=duration)
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 